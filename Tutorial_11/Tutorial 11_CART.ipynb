{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZF5tpo61XQZ"
   },
   "source": [
    "# Tutorial 11 Ciencia de Datos (UdeSA) 2025\n",
    "## CART: Árboles de regresión y clasificación \n",
    "\n",
    "**Objetivo:**  \n",
    "Que se familiaricen con los arboles de decisión para regresión y clasificación\n",
    "\n",
    "### Temario:\n",
    "- Árbol de decisión y clasificación\n",
    "- Ejemplo práctico con base de Titanic para predecir sobrevivientes: https://www.kaggle.com/dmilla/introduction-to-decision-trees-titanic-dataset\n",
    "- Ejemplo práctico con la base de Hitters para predecir salarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIIk2TrO1XQc"
   },
   "source": [
    "## ¿Qué es un árbol de decisiones? ¿Cómo funciona?\n",
    "El árbol de decisiones es un tipo de algoritmo de **aprendizaje supervisado** (que tiene una variable objetivo predefinida) que se utiliza para problemas de **clasificación y de regresión**. Funciona para variables tanto categóricas como continuas. \n",
    "\n",
    "En esta técnica, el objetivo es dividir a la población o muestra en dos o más conjuntos (o subpoblaciones) homogéneos en función del atributo más significativo en las variables x. Entonces la pureza del nodo aumenta con respecto a la variable objetivo. El árbol de decisión divide los nodos en todas las variables disponibles y luego selecciona la división que da como resultado los subnodos más homogéneos.\n",
    "\n",
    "### Tipos de árboles de decisión\n",
    "Los tipos de árbol de decisión se basan en el tipo de variable objetivo que tenemos. Puede ser de dos tipos:\n",
    "\n",
    "**1. Árbol de decisión de variable categórica:** Árbol de decisión que tiene una variable de destino categórica y luego se llama como árbol de decisión de variable categórica. Ejemplo: Titanic\n",
    "\n",
    "**2. Árbol de decisión de variable continua:** El árbol de decisión tiene una variable de destino continua, por lo que se denomina Árbol de decisión de variable continua. Ejemplo: Hitters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MG8NaLVS1XQd"
   },
   "source": [
    "### Terminología importante relacionada con los árboles de decisión\n",
    "Veamos la terminología básica utilizada con los árboles de decisión:\n",
    "\n",
    "**1.Nodo Base/Raíz (RootNode):** Es el primer nodo. Representa la muestra completa y esto se divide en dos o más conjuntos homogéneos.\n",
    "\n",
    "**2.Dividir (Splitting):** Es un proceso de dividir un nodo en dos o más subnodos.\n",
    "\n",
    "**3.Nodo interno o nodo de decisión (Decision Node):** Cuando un subnodo se divide en otros subnodos, se denomina nodo interno o nodo de decisión.\n",
    "\n",
    "**4. Nodo Terminal/Hojas:** Los nodos que no se dividen se denominan Hoja o nodo Terminal.\n",
    "\n",
    "**5.Poda (Pruning):** Cuando eliminamos subnodos de un nodo de decisión, este proceso se llama poda. Es decir, el proceso opuesto de la división.\n",
    "\n",
    "**6 Rama/Subárbol:** Una subsección de todo el árbol se llama rama o subárbol.\n",
    "\n",
    "**7.Nodo padre e hijo:** Un nodo, que se divide en subnodos, se denomina nodo padre de subnodos, donde los subnodos son los hijos del nodo padre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo de la base Hitters"
   ]
  },
  {
   "attachments": {
    "Tree_Hitters.JPG": {
     "image/jpeg": "/9j/4AAQSkZJRgABAQEAYABgAAD/4RDmRXhpZgAATU0AKgAAAAgABAE7AAIAAAAJAAAISodpAAQAAAABAAAIVJydAAEAAAASAAAQzOocAAcAAAgMAAAAPgAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAFZpY3RvcmlhAAAABZADAAIAAAAUAAAQopAEAAIAAAAUAAAQtpKRAAIAAAADNjIAAJKSAAIAAAADNjIAAOocAAcAAAgMAAAIlgAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADIwMjM6MTE6MDYgMDk6MDg6MzAAMjAyMzoxMTowNiAwOTowODozMAAAAFYAaQBjAHQAbwByAGkAYQAAAP/hCxtodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvADw/eHBhY2tldCBiZWdpbj0n77u/JyBpZD0nVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkJz8+DQo8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIj48cmRmOlJERiB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPjxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSJ1dWlkOmZhZjViZGQ1LWJhM2QtMTFkYS1hZDMxLWQzM2Q3NTE4MmYxYiIgeG1sbnM6ZGM9Imh0dHA6Ly9wdXJsLm9yZy9kYy9lbGVtZW50cy8xLjEvIi8+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iPjx4bXA6Q3JlYXRlRGF0ZT4yMDIzLTExLTA2VDA5OjA4OjMwLjYxOTwveG1wOkNyZWF0ZURhdGU+PC9yZGY6RGVzY3JpcHRpb24+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczpkYz0iaHR0cDovL3B1cmwub3JnL2RjL2VsZW1lbnRzLzEuMS8iPjxkYzpjcmVhdG9yPjxyZGY6U2VxIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+PHJkZjpsaT5WaWN0b3JpYTwvcmRmOmxpPjwvcmRmOlNlcT4NCgkJCTwvZGM6Y3JlYXRvcj48L3JkZjpEZXNjcmlwdGlvbj48L3JkZjpSREY+PC94OnhtcG1ldGE+DQogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgIDw/eHBhY2tldCBlbmQ9J3cnPz7/2wBDAAcFBQYFBAcGBQYIBwcIChELCgkJChUPEAwRGBUaGRgVGBcbHichGx0lHRcYIi4iJSgpKywrGiAvMy8qMicqKyr/2wBDAQcICAoJChQLCxQqHBgcKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKir/wAARCAGNA1ADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD6RooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACg9KKKAMCw8WWlw2sR36nT5dHkIuUmYcR43LKD/dYfyI7Va0HWhrWj2l9JbvZNdJ5kdvOw8wJn5SR2yMHHbNcj8QNCtb7xd4Y3tJGupXRsr5IzgXUCo0yo/qN0Y/BmHeuSv4pp7/VYb19Jt9ffU2+yyy+ab6NRIPJ8pVX7m3HCnbjOe9C1B6HtrTRJIqPIiu33VLYJoM0YbaZFDZxjPNeK+Op7aePxRN9n06DVLUhY3uleW9ZkRSHgHHlp1wRkcMT3rofsGnat4p8W6hP5kxh061a2uLceY8O6GTMkQ/v46Ec0dLh1sekpNHJny5FfacHac4NPzXk3gzUdL0jVJHsYdMvY4dKeSa80ZWj4Qqds8ZJHmHsS2eG6Cus8San4ghu4BYW8kOkSRbpr61hFxcRn0EeeOO4D/Sh6CTudbmsi88S6dZeJNP0GWUnUL9XeKNBnaqDJZvQcYHrR4fktZNER9Nv5tSjJY+fcSFnZs8g5AxzxjAx6V5wieIbLx34dudW0AHUbu8uHmnF7GQy+UQFXuFReg789zT62Dpc9I8Q+IIfDmmreXFtdXfmTJBHBaIGkkdzgAAkD9adoWtSa1bySy6TqOlmN9ojv41Rn4zkBWbiuf8AF93oGrWdr9v1S6gtrDVFWa4sX2i3mVTxI+MoPm6jGCRyKd4Hu5Z7/WIrPUbjVNEikj+xXdxKZSWK/vEEh5dQcc5PUjPFJDZ2VFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFZk9/f8A9qy2dhaW8oihjlZ5rlo/vs4AACN/c/WtOs2D/kab7/rytv8A0OepZMr6B5+uf9A/T/8AwOf/AOM0efrn/QP0/wD8Dn/+M1pUUWfcLPuZvn65/wBA/T//AAOf/wCM0efrn/QP0/8A8Dn/APjNaVFFn3Cz7mb5+uf9A/T/APwOf/4zR5+uf9A/T/8AwOf/AOM1pUUWfcLPuZvn65/0D9P/APA5/wD4zR5+uf8AQP0//wADn/8AjNaVFFn3Cz7mb5+uf9A/T/8AwOf/AOM0efrn/QP0/wD8Dn/+M1pUUWfcLPuZhl1okE6dp5I6Zvn4/wDINJ5msltx03Ttw7/bnz/6JrUoos+4WfcyzJrJbJ03TicYz9uf/wCM0CTWl+7punD6Xz//ABmtSiiz7hZ9zl/Eb6svhfVd2nWCKbWXcUvHJ+6eceUM1oW02t/ZYsafYEbBjN8/p/1xq/qNkmo6bc2UrMqXETRMy9QCMcVNEgjiVByFUAUWd9yeV817meJtbHTTtPH/AG/P/wDGaDLrRIJ07TyR0/05+P8AyDWnRRZ9yrPuZfmazgj+zdOweo+3Pz/5BpVl1pRhdO08D0F8/wD8ZrToos+4WfczfP1z/oH6f/4HP/8AGaPP1z/oH6f/AOBz/wDxmtKiiz7hZ9zN8/XP+gfp/wD4HP8A/GaPP1z/AKB+n/8Agc//AMZrSoos+4WfczfP1z/oH6f/AOBz/wDxmjz9c/6B+n/+Bz//ABmtKiiz7hZ9zN8/XP8AoH6f/wCBz/8Axmjz9c/6B+n/APgc/wD8ZrSoos+4WfczfP1z/oH6f/4HP/8AGaTz9c/6B+n/APge/wD8ZrToNFn3Cz7lDTL25upLuG8t4oJbaQIRFMZFYFFbOSq+vpV+s3Tv+Qtq/wD13j/9FJWlTjsEdgoooplBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFYM8d1feI7i2S/ntoo4UcCIjkn61vVj23/ACN17/17R/zNAB/Yl1/0Gr381/wo/sS6/wCg1e/mv+FbFFAGP/Yl1/0Gr381/wAKP7Euv+g1e/mv+FbFFAGP/Yl1/wBBq9/Nf8KP7Euv+g1e/mv+FbFFAGOdEusf8hq9/Nf8K5bwTcX2q+Dzqmr6/dIwuJ0ZyVAAWQqO3tXoJrz34c6dDqvw0NrcFgj3lwcqeQRM1YVXNfw97O3roRp7aKk9Nf0OlttOe8gWa1168ljboysuD+lS/wBiXX/QavfzX/Crek6ZDpFitrblmUEksx5JNXauk5uCdRWl19TWXKpPl2Mf+xLr/oNXv5r/AIUf2Jdf9Bq9/Nf8K2KK0JMf+xLr/oNXv5r/AIUf2Jdf9Bq9/Nf8K2KKAMf+xLr/AKDV7+a/4Uf2Jdf9Bq9/Nf8ACtiigDH/ALEuv+g1e/mv+FH9iXX/AEGr381/wrYooAx/7Euv+g1e/mv+FH9iXX/QavfzX/CtiigDH/sS6/6DV7+a/wCFH9iXX/QavfzX/CtiigDH/sS6/wCg1e/mv+FH9iXX/QavfzX/AArYooAx/wCxLr/oNXv5r/hR/Yl1/wBBq9/Nf8K2KKAMf+xLr/oNXv5r/hR/Yl1/0Gr381/wrYooAx/7Euv+g1e/mv8AhR/Yl1/0Gr381/wrYooAx/7Euv8AoNXv5r/hR/Yl1/0Gr381/wAK2KKAMf8AsS6/6DV7+a/4Uf2Jdf8AQavfzX/CtiigDH/sS6/6DV7+a/4Uf2Jdf9Bq9/Nf8K2KKAMf+xLr/oNXv5r/AIUf2Jdf9Bq9/Nf8K2KKAMf+xLr/AKDV7+a/4Uf2Jdf9Bq9/Nf8ACtiigDH/ALEuv+g1e/mv+FH9iXX/AEGr381/wrYooAxtDa4W91G2uLmS4EEiqjSHnBXNbNY+k/8AIa1j/rqn/oNbFAHOP4109J5Y2gvdq+Z5UggytwY2CuseDkkMQOQM9sio5vHWnwW3mSWt8JUaUTW4iUyQiMKzs3zYwAyngnOeM1XtvD2vpqV7e3F9YyXMu8W9wY3Zok3giMKTtC4ABxgnrmsy5+HMtxahHbTnDSzP9jeB/s8JkVBujAYEMNpPXq7dKA6noEUqTRJJGdyOoZT6g06oLKBrayggkmad4o1RpX+85Axk+5qem99BK9tQooopDCiiigArNg/5Gm+/68rb/wBDnrSrNg/5Gm+/68rb/wBDnqXuiXujSoooqigooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKDRQaAM3Tv8AkLav/wBd4/8A0UlaVZunf8hbV/8ArvH/AOikrSqY7Ex2CiiiqKCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKx7b/kbr3/r2j/ma2Kx7b/kbr3/AK9o/wCZoA2KKKKACiiigAzRXO339vf8JRB9kz9gyu7GNuP4s9810QrClW9rKS5WuV216+a8jScOVJ3vcDXE/CX/AJENP+vy5/8ARrV2xrifhL/yIaf9flz/AOjWqn/EXo/0OWX8WPo/0O2ooorU2CiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDH0n/kNax/11T/ANBrYrH0n/kNax/11T/0GtigAooooAKKKKACiiigAooooAKzYP8Akab7/rytv/Q560qzYP8Akab7/rytv/Q56l7ol7o0qKKKooKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACg0UGgDN07/kLav8A9d4//RSVpVm6d/yFtX/67x/+ikrSqY7Ex2CiiiqKCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKx7b/AJG69/69o/5mtise2/5G69/69o/5mgDYooooAKKKKACjNFIRmgCraapZX8skdpcJK8Z+cKelcp8Jj/xQaf8AX5c/+jWre0nw5Z6NdSz2pkLSDADHIUegri/BkksXwjle3uBbuLy4+cnH/LZuAfWvOVatTp+0rxXMlJ2Xl6jnCEsTCNN6O+/yPTAc0Vi+FJJptAhe4uBcOSfmzkgZ6E+tQ+I7zWrW4tRo8JeNj85Cbsn0PoPetHi4rDLEOLs0nZavU29i3VdO50FFNQsVG8YOORTq7TAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAx9J/5DWsf9dU/9BrYrH0n/kNax/11T/0GtigAooooAKKKKACiiigAooooAKzYP+Rpvv8Arytv/Q560qzYP+Rpvv8Arytv/Q56l7ol7o0qKKKooKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACg0UGgDN07/kLav/13j/8ARSVpVm6d/wAhbV/+u8f/AKKStKpjsTHYKKKKooKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArHtv+Ruvf8Ar2j/AJmtise2/wCRuvf+vaP+ZoA2KKKKACiiigAoJxRSEZoAhhvLa5d0t545WjOHCOCV+tcN8NtPg1P4bfZboExveXPQ4IPnNzXSaJ4Zh0W7mnjneQyDaAwxtGf1rlfAIv2+FrjSji5+2XG3GM485s4z3rzeepKlzYiGtpXitbr/AII5KKxMFTl3127Hd6bpsGlWa2tqD5aknLHJJPc1brN0EaiukxjVzm4yeuM47Zx3rSrsw/L7KPLHlVttreRVS/O7u4UUUVsQFFFFABRRRQAUUUUAFFFFABUE95bWpUXM8cW84Xe4G4+2anrE13w3FrksEkkzxGLIO0ZyP6VhiJVY026MeaXbY0pqDlabsjbHNFIihI1UdFGBmlrczCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAMfSf+Q1rH/XVP8A0GtisfSf+Q1rH/XVP/Qa2KACiiigAooooAKKKKACiiigArNg/wCRpvv+vK2/9DnrSrNg/wCRpvv+vK2/9DnqXuiXujSoooqigooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKDRQaAM3Tv+Qtq/wD13j/9FJWlWbp3/IW1f/rvH/6KStKpjsTHYKKKKooKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArHtv8Akbr3/r2j/ma2Kx7b/kbr3/r2j/maANiiiigAooooAKKKKAA1xPwl/wCRDT/r8uf/AEa1dsa848B6mdI+Fpu0j8xlvbgKpPGTM1c9apGk/aT2Sb/Iz5XOtGK3d/0PR6Ky/D+qtrOlrdPH5bbirAHjI9Kp+I9P1e9ubZtKuTEiffAkK8+vvUTxS9gq9KLknayW+p1Kl+85Jux0FFNQMFAY5OOTTq6zEKKKKACiiigAooooAKKKMigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAMfSf8AkNax/wBdU/8AQa2Kx9J/5DWsf9dU/wDQa2KACiiigAooooAKKKKACiiigArNg/5Gm+/68rb/ANDnrSrNg/5Gm+/68rb/ANDnqXuiXujSoooqigooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKDRQaAM3Tv+Qtq/8A13j/APRSVpVm6d/yFtX/AOu8f/opK0qmOxMdgoooqigooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACse2/5G69/69o/5mtise2/5G69/69o/5mgDYoqvfSywWU0tvH5sqISif3jjgVmeGdSv9StJX1KDymV8IdhXcPoawliIRrRou93d7aaeZoqbcHPojbooorczCiijNAAa818EQ3M/wnljso1lla7uBtYA5HnNng969JzXE/CdgvgFSxwBeXOSf+urVzV6aq+43a6a/IhSca8JLpf9DoPDMF1baHDFexLFIpOFAA4zxkDvVLxP4juNEureK3hRxIu5i+emcYFdGrBhlSCD0IrhvH7gahZDewwhJAHTnr9a83MZTweX/upWcbK/XdI9HDJVsR763udxG++NWxjcAcU6o4TmFDknKjk96hTUrOS+azS4ja4XrGDyK9jnjFLme5x8rd7ItUU13WNC8jBVUZJJ4AqGzv7XUIzJZzpMqnBKnoablFS5W9RWdrliikZ1RcuwUepOKXOaoQUUUh6UAKa8G8PX/gB9InuPFGv6hFqy3dz5pXU7xSmJnC4VW28DHavZNM8QWOrXUtvaMxeMZ+YYDDpkVn+CNBudE8Jrp+qRR+d9ouHZQQwKvK7D9GFZU6sK0eem7ouUZQfLJWF+H02pT+AtKl1uVprt4iTJIQWdNx2Fscbim3PvXSV5Yvh/+zfBenaJq7266lYwXRtbKe8aK3lUScSb1B5VSuB2yfqN+y8V/wBm+E9Akljnu3ubKJ3kmOHb5BknrknrRiK9OhB1ajtFfqKnTlOShFanaUm4ZxkZ9KRG3xqwGNwzzWBb6BexeKpNSa83QMSQmTkg/wAOOmB/Soq1JwceSPNd6+S7lQjGV+Z2sdDRRRXQZhRRRQAUUUUAFFNV1YkKwJXggHpTqACiiigAooooAKKKKACiiigAooooAx9J/wCQ1rH/AF1T/wBBrYrH0n/kNax/11T/ANBrYoAKKKKACiiigAooooAKKKKACs2D/kab7/rytv8A0OetKs2D/kab7/rytv8A0Oepe6Je6NKiiiqKCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoNFBoAzdO/5C2r/wDXeP8A9FJWlWbp3/IW1f8A67x/+ikrSqY7Ex2CiiiqKCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKx7b/kbr3/r2j/ma2Kx7b/kbr3/r2j/maANjFGMUUUAFFFFAFKbV7KDUEspbhVuJPuoc/hVs1l3Ph2yutZj1KXf50ZBwG+UkdCa1COK56TrOUvapWvpbt5mk+Sy5fmcn4Rub2fVL5bq/W4Vf4d+7Jz1HoKx/AGnvqnwua0jl8pnvLghu3EzcGrvge2SLV70rcRSbF2DZn5hnqPaqvw71KPSfhmbuZWdUvLgbV6kmZq8XL3GeCXt9rTvrfS/c1xaaxsfZ72/yOx0HTH0nSo7WWbzmUkluwyegqvrq6GZrf+2vL35/dbif6dvrxVzR9Ui1fT1uoUZASQVbqCK5rxyE+3WG60eY92BI3DP3a6sVUpUcApUkpRVrXu1bT5l0YyniLTbT12OxUDaNvTHGKyYPDdpb68+qI0hkYlthPygnqa1Yv9WuBtG0celPr0p0adXlc1ezuvJnNGcoXUXuRXVul1ayQS52SKVbB7GqGiaFb6JFIlu7uZGBZnPp0FXb1p1spjaKGnCExg9zjisrwxNq01rMdZRlYP8Auy67WI78VhU9l9aheDcrOztovmaR5/ZSs9OxJ4k0aTW7GOGGcQsj7uRw1XtPtjZafBbNIZDEgUue9UfEurz6NYJNbQCVmfaSwOF+uKvafcteafBcSRmJpEDFD2qYfV/rc+X47K++3TyCXtPYq/w3LVI33a5vUfE0tl4mh01YEaNyoZifm+b07V0hGRit6OIp1pTjB6xdn6kTpygk5dTm/Dljo1tqV0+l3RnlA2sp/gXPbjnkDmulrJ0vw5ZaRdS3Fpv3SjGGbIUZzgVrVlgaUqNHklFRd3oti681Od02/Uq32mWOpxqmo2VvdojblWeJXCn1AI61X1G80uxMCai0K5P7oOmcfTjj61pVlat4fs9Zkhe73houmxsZHoa1xPtfZP2KTl57EU+Tm996eRqAgqCOnalpFUIgUcADApc10GYUVFdXCWtpLPLnZGpY4HOBWfoWvQ65DK8MTxGJgGVuevSsZV6caipN+89l6FqEnFzS0Rq0UUVsQFB5U0UUAc9oXhybSNTubqW785ZQVC4POTnJ9+P1roR0oorDD4enh4ezpqy/zNKlSVSXNLcKKKK3MwooooAKKKKACiiigAooooAx9J/5DWsf9dU/9BrYrH0n/kNax/11T/0GtigAooooAKKKKACiiigAooooAKzYP+Rpvv8Arytv/Q560qzYP+Rpvv8Arytv/Q56l7ol7o0qKKKooKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACg0UGgDN07/kLav/13j/8ARSVpVm6d/wAhbV/+u8f/AKKStKpjsTHYKKKKooKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArHtv+Ruvf8Ar2j/AJmtise2/wCRuvf+vaP+ZoA2KM1Be27XVjNDHIYmkQqHH8OR1rL8M6LPotpLFcTiUu+4Bc4X/wDXWEqlRVowULxd7u+3yNFGLg5N69iXX9cXQrWOVoWmMj7QoOP1q9Y3a31jDcxgqsqBgD1FUte1DT9PtFbVIvNR2wqbA2T681esp4bmzimtv9U6gpxjj6VlCcniZR501Ze71XmVKK9kny/MyrzxNFZ+IItLNu7GQqpkDdC3TjvW31qJ7SCS4Wd4Y2lThXKgkfjUw4rWlGrGUnUldN6abLsTNwaXKrdzM03QbHSriWazjZXk4OWzgegri/Ad3BZfCt5ruD7RF9suAYj0bMzV6NmuH+FUST/D8Ryoro15cgqwyD++asZUIwj7OilHR2001t0MnUbxEZT13/Q6Xw9eW99o8ctnb/ZogSvlDopz+tYnjPV73Tr20Szl8tSpc/KDk5rrIYY7eNY4UWNF6KowBXLeMdWl0+8s0jtoZR9/Mqbuc9B6VxZhzUsvtKfK1bVLzXRHdh7TxF1G610Z1MTFolY9SATT81Gjb41YjBIBx6Vxun2t+vxAmleKYRb3y5B2lO3PTHSu3EYl0HTSjfmaXp5mNOl7RSd7WVztqMU2SRYoy8jBVUZJJwAKgstRtdQjL2UyzKpwSp6GutzipKLerMeV2uWCoYYYZHvRjioLy/ttPhEt5MsSE4BbualjlSeJZImDo4yrA5BFLni5ct9Qs7X6HO6l/YX/AAlFv9s3/bvl24ztz/DmulrjtXhnbxvaSJpvmoNmZNpweepPTiuxJwM15+Ck3UrXilaXRWv69zprq0Ya30CisrS/EVjq11Lb2hffGM/MuAwzjIrVrupVadaPPTd0c8oSg7SVmFFFc94kg1uaa2/saQqgJ3hWA57E+1TiKzo03NRcvJblU4c8uW9vU6HtXK2t9qj+N5rWSdDbLk+WGXhcccdc11CbhEu/G7HOPWuRtNAvovG0l+8ai2MjyB93XOePrzXFj/a81H2afxK9u3n5G2H5LT5rbHXsquhVwGUjBBHWobWytrKMpaQJCpOSEGMmp6K9Lli3zNanNd2sFFFFUIKKKKACijNFABRRRQAUUUUAFFFFABRRRQAUUUUAY+k/8hrWP+uqf+g1sVj6T/yGtY/66p/6DWxQAUUUUAFFFFABRRRQAUUUUAFZsH/I033/AF5W3/oc9aVZsH/I033/AF5W3/oc9S90S90aVFFFUUFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABQaKDQBm6d/yFtX/67x/+ikrSrN07/kLav/13j/8ARSVpVMdiY7BRRRVFBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFY9t/yN17/wBe0f8AM1sVj23/ACN17/17R/zNAGxRVe+M62MxswDOEPlg/wB7HFZnhmXV5bSU62pDh/3ZZQCR34FYSrqNaNLlet9baad2aKF4Od9vvK3jL+z/AOz4f7TEx/efu/Jxuzjnr2rX0f7P/ZFt9i3eR5Y2buuPeqviHQhrtpHF55haN9wO3IPGOlX7C0WxsIbVGLiJAoY965KdKqsbOo4rlaVn1NZTi6EYp632IJtZsLfUksZrhVuHxhMHv0GelXia5rUYNCbxXA11K63xZSEGdpb+HNdKBW2Hq1Kkqim1o7Kz/PzIqQjFRavquv6HF+DL+/u9Wu1vJ5pVCZIkJIVs/p34ql8OdRi0r4afargMUS8uBhRySZmr0AIq5KgDPXA61w3wutorv4d+TcRrJG95chlYcH981c9DDVcPSVJT5pWlZvzM61WNTExm1ZWen3HXaTqkOr2K3VuGVWJBVxyCKtSQxSlTLGjlTldyg4NMtrWGzgWG2jWONeiqOBVbUdbsdKkjS9m2NL90bSfx4rs5vZUU8RJdLvZXKtzTtTRfxRikDblBHIIyKhW8t3ujbrNGZlGTGHG4fhWzkluyLNkeqxPNpNzHHH5jtEwVM/eOOlYHgeyubOzuvtVs0JdxtLjBbA9P89a6qiuWphYzxMMQ3rFNfeaxquNN0+5na1o1vrVskNyzpsbcrIeRVqztY7K0itoc+XEu1cnms3xLJqsdgh0VSZd/z7VBIGOwNX9Pa5bT4DfAC4KDzAPWlB0vrUrQalZXlbR+VwlzeyV3pfYtcUjfdNc9qGk6rceJoLy3u9lqm3cu8jAHUY75roW+7WtKrKcpKUWrOy8/NEyiopNO9/wOS8KW97Dq9811p62yN/EExg56D1FddXO6BpOq2Go3Muo3fnRSDCjeTk5647Vp63a3N7pM0FjL5UzY2tnHfkZ7VxYFToYT4XdXdna+/l3N67U6266al+jFZegWd3Y6THBqEvmzAk53ZwPTNalejSm5wUpKzfTsc0koyaTuFRiaEzGISIZAMlAwyB9KkPSuat9EtY/F0t6uoq0xJf7PkbgT689PwrKvUqQcFCN7uz1tZd/P0KpxjJPmdrI37x5Y7KZ7ZQ0qoSinoTjisbwrqOo6jZzvqQHyvhGCgZ9Rx+FbN3bi7spbdmKiVCpYdRkVmeHtAGhQTJ9oMxlYEnbgDHtmsasK7xdOUb8iTvrp5adTSLh7GSe+ljZooorvOcxr3xLaWWsR6dKshkfaCwA2rnpmtjkr71zep6ppcHia3gubLzbkbQJ8fcz0+tdKOlcWGqyqTqJzTs7adPJm9WCjGLStdfec9oNhrVrql1Jqly0kDAhAX3ZOeCB24zXQ0UVth6EaEOSLb9XcipUdSXMwooorczCiiigAooooAKKKKACiiigDH0n/AJDWsf8AXVP/AEGtisfSf+Q1rH/XVP8A0GtigAooooAKKKKACiiigAooooAKzYP+Rpvv+vK2/wDQ560qzYP+Rpvv+vK2/wDQ56l7ol7o0qKKKooKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACg0UGgDN07/kLav/ANd4/wD0UlaVZunf8hbV/wDrvH/6KStKpjsTHYKKKKooKKKKACiiigAooooAKKKKACiiigAooooAKD0opk0qQwvLM6xxopZnc4CgdST2FAHKar40ntb+/sNP09Z7mC4gs4PNn2CaeVQ2OASEVDktz0PHFbHhvWW13R1u5YRBMsskM0avvUPG5RtrYGRlTg4HFecXk+qL4puNR1S6+H8d5sC201zdskohZB0PXBBPJ9Tjiu+8FW32XwpaRCfTp0G8o2mD/R1UscKh7gdMnknOaFsD3N+se2/5G69/69o/5mtise2/5G69/wCvaP8AmaANijpRWR4svrjTPBusX1k/l3FtYzTROVB2sqEg4PB5FJuyuNK7sGv64uh2scxhMxkfaADgDv1q/YXQvrGG5VWQSoGCt1FeZXw8T2XgE+I7rxml+0dkt39ivNOt/KkbaG2ZVQwJ6Ag969N0+Vp9Ot5ni8lpIlcxf3CRnH4dKxVOqq0pSl7va2z9SnKDgklr3MHUPDNxeeKodSSaMQoyMynO75T0H5V0orKuvEdlaazHpsm/zpCBkLwCegNapPFYYaGHhOo6L1b971NKsqjUeftp6BmvPPh7qS6T8MTdtG0gS8uAFHcmZhV/wZqt9farcpd3DSoY9+GPQ5HSmfCqJJvh+ElVXRry5BVhkH961Y0MV9dpKrR91tSSv0ehlWpewxMYz10f6HUaLqi6xpqXaxtHklSpOeRWF4xg0uW8tDqNzLC+CAI03ZXPU+ldVFDHBGEhRUReiqMAVU1DRrHU5I3vYBI0X3Tkj+XWrxOGq1sJ7J2lLTfZ/cb0qkYVedXS8i1GAIlCHKgDH0ri9P8Asn/CxJ9on373xnGN3f8ADrXbbQq4HArlLKwgTx1NOupRvLlmMAB3cjoT04rHMISlOhZLSa7fhcvDySU79jraKiubhLW2knmOI41LMfYVS0bXLbW4ZHtQ6+W2GVxyPSvSlWpxqKm37z2RzKEnFyS0RpHHekrn/GVxdW2kxvZ3IgPmAN820sMdAa1dJaV9KtmuJVlkMYLOpyGNZRxCliJULapJ36alOm1TVS+5k6gmvHxNbmzJFh8u7BG3H8We9dCTgc0tBGaqlR9lKUuZvmd9enp5CnPmSVrWMvTdfsNVuZYLSRmeMZOVxkZxkVqVQsdFsNNuJZrOARyS/eOSfwHpVq6uYbO3ee5kEcSDLMe1FF1Y0r4hq+u21vmE+RytTvYlrm/FHiC80aa1S0jjYS5LFwTnHat20vIL63We0kWWJujCs7WvDltrckLzySI0XHydx6VhjVWq4Z/VX7ztZ38/8jSjyRq/vVoayEtEpIwSMkelcRZWsy/ESWQ20oj8x23HOAMdc+n+NdwqhECr0UYFG2qxOEWIdNt25Wn9wqVZ01JJbqwvaiiiu0wCiiigCCSxtZblLiW3ieZPuyMgLD8anooPQ461KildpDu3uGaK53QNS1e81S6i1K28qBAdh2bcHPTPfjP5V0VZYevHEQ54ppeasXUpunLlYUUUVuZhRRRQAUUUUAFFFFABRRRQBj6T/wAhrWP+uqf+g1sVj6T/AMhrWP8Arqn/AKDWxQAUVGJozM0QdTIoDMgPIBzgkfgfyqnd69pNgkb32qWVssufLaa4RA+OuMnnFAGhRVK61jTrG0jur2/tbe3lx5c00yoj5GRgk4PHNW4pUmhSWJ1eN1DKynIYHoQe4oAdRRRQAUUUUAFZsH/I033/AF5W3/oc9aVZsH/I033/AF5W3/oc9S90S90aVFFFUUFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABQaKDQBm6d/yFtX/67x/+ikrSrN07/kLav/13j/8ARSVpVMdiY7BRRRVFBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABUF7HJNYzxwiIyPGyqJl3ISRxuHceoqeormE3FrLCsskJkQqJIyAyZHUZzyKAPMb3T/Fr+J7PS7m28FzzXFpJMsz6VIdqxFF28yZ/jGPpXoWhWl1Y6Pb298tks6A7xYQmKEcn7qkkj865d/hpJJqEV8/jTxMbmGNokkM8OVViCw/1Xcqv5V1ul2L6bpsVrJe3N88YINxdMpkfnPJUAfpT6C6lyse2/5G69/69o/5mtise2/5G69/69o/5mkMvakXXTLkxzCFxE22Rjwhx1rh0F7qPwz8SQiZ76aSznjihTLvuMbAADqST0FdtqozpN0DCZx5Tfux/Hx0rA8BqP7NuP8ARzEfM++c/P8An6V5Fe/9oUo9OWXf/hv69DtpaYeT80clqXw38NWPwva9svDtpZa7DpqywTxQCOdLlY9y4PXduHT1rqbjVtWGoeEmhlSKz1B9t1FJCwmZvs8j4OfujKjIxnIrY8QaGNctI4fPMJjfcDjIP4VLHodoItNWbdK+mtvgcsRhtjISfXhjXoxqVXXlGUfc0s79eqscvLBU009dTE1O9hj8aWkL6YksmUAnOd3J6gdOPeurrCvfEtvaeIYtNa2Z2ZlUy5+6W6cd63gK5MHye0q8s+b3tdLW8vP1Na1+WF1bTvucP4IW0GsXn2eaRjsxGGXGVz1+vSuV8F6VrZ8GvqFr4zm0eyF3Pi3WxjlCnzWHU8nJr0/TPD9hpFxLPZoweTg7mztHoK5T4a6fBqfw2+y3QJje8uc4OCP3zciuXCYWrh8OqTS5lzWs3bfTzMsXKnXxUW72t6Pp2FsvD3ijULVbi0+I1xJG3QjTIay9f8NeO4J7cW3i++vVJzujsY02N77e3ua9I03TbfSrNba0BEaknLHJJPc1axmuyWGnVoKM3yy0vZ31+dzNU6UKl43a9X+jOHXwl4xKjd8QbkHHI/syHis2H4f6ymsNPD4+kF9ku2LCEsM9TjPv6V6SRXGWOk6jH47lu3gYQeY7eYT8pU9Ofy4rHGKMJUl7PnvJLrp5l0cPCSl7zWnd6+W5DP4L8W3EDwzfEC5dHUqy/wBmwjIqrpnw88T6ZE623jqaEucsE0+JgcdOteiUV2vDUnNTa1XXUxVNKPLd29X/AJnn1/8AD7xJqcaJfePLiZUO5QdNiGD+Bqa38E+K7W3SC38f3KRxjCr/AGbCcCui8TNqq6en9ihjJv8An2AFse2a0NONydPg+3AC42DzAPWsIxpfWJRUGnZe9rr5XNHQXs0+Z27cz/zOS/4RHxh/0UK5/wDBZDR/wiPjD/ooVz/4LIa7aiur2UfP73/mY+xj5/e/8zif+ER8Yf8ARQrn/wAFkNUtX8IeIjpkv9q/EG4NqMFs6bD68dOa9Dqtf2EGpWT2t0CY364OCKzrUVKnJRV3bq3b5+RcKUFJNt/e/wDM4HRfCGv/ANmqdH+INwLZmJGNNi4PfrzWj/wiPjD/AKKFc/8Agshrq9N0230qzW2tAwRSTljkk+pq3U0KHLSipKztsm7fIKlKDm2m/m3/AJnE/wDCI+MP+ihXP/gsho/4RHxh/wBFCuf/AAWQ121V1vrZrs2qzxmdRkxhvmA+lauEFu/xf+ZKoRe1/vf+ZyP/AAiPjD/ooVz/AOCyGj/hEfGH/RQrn/wWQ121FP2UfP73/mL2MfP73/mcT/wiPjD/AKKFc/8Agsho/wCER8Yf9FCuf/BZDXbUUeyj5/e/8w9jHz+9/wCZxP8AwiPjD/ooVz/4LIaP+ER8Yf8ARQrn/wAFkNdtRR7KPn97/wAw9jHz+9/5nE/8Ih4w/wCihXP/AILIaP8AhEfGH/RQrn/wWQ121FHso+f3v/MPYx8/vf8AmcT/AMIj4w/6KFc/+CyGj/hEfGH/AEUK5/8ABZDXbUUeyj5/e/8AMPYx8/vf+ZxP/CI+MP8AooVz/wCCyGj/AIRHxh/0UK5/8FkNdtRR7KPn97/zD2MfP73/AJnE/wDCI+MP+ihXP/gsho/4RHxh/wBFCuf/AAWQ121FHso+f3v/ADD2MfP73/mcT/wiPjD/AKKFc/8Agsho/wCER8Yf9FCuf/BZDXbUUeyj5/e/8w9jHz+9/wCZxP8AwiPjD/ooVz/4LIaP+ER8Yf8ARQrn/wAFkNdtRR7KPn97/wAw9jHz+9/5nK+DdP1DTr7WItV1aTVZvOT988KxcbfRa6qsfSf+Q1rH/XVP/Qa2K0SsrI1SSVkczaaDHH4u16V7TFpqNrArtniVwZd/fPRl/OqFpol0ngnSNDNjsDyeTckgHyIcszf99ABeP71dpijFMfW5yPieze9a08iy1RF0+V1jayWLqY8KQr5yvzEdsEeldHo4vF0SyGp7ReCBBcbPu+ZtG7GPfNXMUUAFFFFABRRRQAVmwf8AI033/Xlbf+hz1pVmwf8AI033/Xlbf+hz1L3RL3RpUUUVRQUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFBooNAGbp3/IW1f8A67x/+ikrSrN07/kLav8A9d4//RSVpVMdiY7BRRRVFBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFY9t/wAjde/9e0f8zWxWPbf8jde/9e0f8zQBqXFxHa28k8x2xxqWY+gFU9J1q01mF5LMviNsMHXBFXZYo54nimUOjgqynoRVfT9MtNLhaOxiEasctzkk1hJVvbRcWuTW/e/SxouTkd9zO8T6nfaZZxyadCJCz4dipbaMenvWlps01xp0E1zH5UroC6Y6Gp5JI4l3SuqL6scCnDBGQcg1MaUlXlUc7ppe70XmDknBRt8zEvNS0eLxDDb3MQa9JASTywdpPQZrbFUptHsLjUUvpbdWuI8bXz6dKunpRRjVjKbqWs3pbt5+Y5uLS5b/ANdhA6sSFIJHXB6V558PtSGk/DA3ZjMuy8uAFBxkmZu9T+BHlOtXQfecxEsSe+R19+tVvAd8un/Ct7hoBcBby4HlkcHMzdfavOo476zhvrC9zSXna3XzCtQ9li4U9/6R22iaoNY01LsRGLcSpUnOCPQ96yvFPiG70a7to7SNGWQbmLjOeelaXh6/XUtHinS3FuMldij5Rg9vatCW3hnKmaJJCpyu5QcH2rpcauIwcVTqWk0vet+nmaXhTrPmjougsbF41YjBIBx6VgW0WujxXK87k6fk4GRt29sD1roenSqceq2UuotYx3CG5XrH3+lbV6cJOHPNxs9NbXfbz9DOnJpSsr6fcTXsskNlNJBH5kqISif3jjpWV4Z1O/1K1lfUoPKKPhG2Fd3rwfStzGaMVcqU3WjUUmkr6dGJSSg4217hRVW/1G102ES3syxITtBPc1NFMk8KSwuHjcZVh0IrVTi5cievYjldr20JKKyLvxJZ2esRadKJPNkIG4L8qk9M1r1NOtTqOSg7taPyZUoSik2twqtqF/Bptk91dEiNOu0ZJqzUN3aQX1q9vdRiSJ+qmqqc/I+Tfpfa4o25lzbEWm6lb6rZrc2hYoxIwwwQat1XsrG30+1W3tIxHGvQCor3VrLTmjW9uFiMh+UN3rOM3TpJ12k+r2V/mNpSm1TRdrDh8MQQ+IX1VZnLMxbyz0DHrzW4DkAjkGinVoU6zi5q/K7r1CNSUL8r3IbuSSGylkgj8yREJVP7x9KyPDOp6hqdtM+pQeWUcBG2Fd3rwfSte7n+zWc0+wv5aFto6nHasrw5rr65bzPJb+SYmA4OQc1zVZxWLpx52m0/d6P/AIY1jFulJ8vbXsbdFFFd5zmXd+IbCy1SPT52cTSY5C5Vc9MmtSqM+jWFzfx3s9urzx42ufbp9avVhSVbml7Vq19LdvM0nyWXL8wooorczCszVdfstGeJbwvul6BFzgeprTqlf6PY6m0bXsAlMZypJx+FYYhVnTfsLc3nsaU+Tm9/byLiOsiBkOVYZB9aWkACgBRgDoBS1uZhRRRQAUUUUAFFFFAGPpP/ACGtY/66p/6DWxWPpP8AyGtY/wCuqf8AoNbFABRRRQAUUUUAFFFFABRRRQAVmwf8jTff9eVt/wChz1pVmwf8jTff9eVt/wChz1L3RL3RpUUUVRQUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFBooNAGbp3/IW1f/AK7x/wDopK0qzdO/5C2r/wDXeP8A9FJWlUx2JjsFFFFUUFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVj23/I3Xv/XtH/M1sVj23/I3Xv8A17R/zNAGhfRzTWM8dtJ5czIQj/3Tjg1meGbDUbCzlTVJ/NZnygL7to+vvWnfQyT2M8UEnlyOhVX/ALpx1rM8M6Ve6VaSx39wJWd8qAxYKPqfWuCpFvFwlyvZ630Xqjoi/wBzJXXT1KHjyCWbS4GiGVSX5vmA6jjrW3ocUkOh2kc331iUHnP61l+MorGXToft9y8GJPk2Lu3HHPFa+kJDHpNslrIZYRGNjnqRXPRppZlUn/dXVfluaTl/s0V5syb/AEzV5vFEF3bXW2zQruXeRgDqMd810PSlxSGu+lQjSlKUW/ed9X+RzzqOaSfQYkcaMxjRVLHLFRjJrz74faZ/a3wva080xF7y4wwGekzHpXR+HtF1LTL65lv7sTJIMABicnPU56VzHgXU5NI+FZuoUV3F7cKobpkzNXHKrGdBzxEOVWldeXyJlFxxUFTd30f3HcaJpX9j6Ylp5xmKkktjHX0FUPEmmatf3Ns2l3PlIn3xvK4Pr71b8OarLrGkrczoqybirBehx3rVrSNGhicJGELqDStbR26GznOnWcpbjUUhAGOTjk+tZcPh2yg1p9TQP5zEnBb5QT1NaxrlrO/1R/Gs1rLcRtbLuPl7l4Xtx1zVYqVKMqaqQ5ryVvJ9xUlNqXK7aanU0VBe+f8AYpvsmPP2Hy89N2OKyvDP9sfZZf7b3bt48vfjdjv0reVblqxpcr1vr0Vu5CheDlfYuavo1trVskN1uARtyshwQas2trHZ2kdvCMRxKFXJzxVHXtbTQ7RJniaUyPtCg49+tXLG7W/sYbmNSqyoGAPUVnGWH+syUbe0sr97dCmqns038PQ5zVb2OLxlaRNpaTP8gExB3cnqO3FdZXOajrN/beJreyhshJA+0FypJIPUg9BiujrHBte0rWlf3u1rf5+pda/LC66d7/8ADBRRRXonMFcd410u9v7u0azt2mUKVJUdDnvXY0YrkxmFji6Lozdk+xtRqujNTQyBSlvGrYyqgHH0p9HasKHxPDN4ifShA4KsU8zPBYdePStKlalR5VN2u7L1JjCU7uK21N08impGsa4RVUdcKMU6itzMKKKKACiq8l/aw3KW8txGk0n3Y2YBj+FWKlSi7pPYbTW4UUUVQgooooAKKKKACiiigAooooAKKKKAMfSf+Q1rH/XVP/Qa2Kx9J/5DWsf9dU/9BrYoAKKKKACiiigAooooAKKKKACs2D/kab7/AK8rb/0OetKs2D/kab7/AK8rb/0Oepe6Je6NKiiiqKCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoNFBoAzdO/5C2r/9d4//AEUlaVZunf8AIW1f/rvH/wCikrSqY7Ex2CiiiqKCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKx7b/kbr3/AK9o/wCZrYrHtv8Akbr3/r2j/maANC+llhsZ5LaPzJVQlE/vHHArM8M6hqGoWcr6nB5TK+EOzbuH0raJA5PFNiminXfBIki5xuRgR+lc8qcnWjPnaST06Pz+RopJQcbfMztc0KHXLeOOaRozG25WX9au2VoljZRW0RJSJQoJ61PRVRoU41XVS956NidSTioN6IyrnxFY2usR6bKX86QgZC8AnoDWpmuC1qNj8Q7bCE5liP3eoyMmu8PSuLBYqpXqVoz2jKy9DevSjCMHHqrmZpfiKx1e5lgtC+6MZ+Zcbh6iuD8KSrF8IJS8Syg3twuH6D983NangRgdZvPnU5jzwuN3zdR6VQ8FxzS/CSVLa3Fw5vLj5MZ/5bNyB61wUsTUxeAdSe7U9l+moqtONHHQitv+GOt8GSrL4dj2RLHtdlIXvz1pfEHiVdDuoIvs5mMg3Md2MDOKl8KxTw6DClzbfZ3BPybcEjPUj1qfVLLS7qWE6osJZWxF5j7cn068/SuuEa/9nwjSkoystWvToXJ0/rEnNXV2aCN5kasOjAGuctfDEtv4rfU2uFMRdnCgHdk9v1rpOABt4Fc1aQaoPGUzy3qNa8nyvNBO3sNvb61tjIQlKlzwcrSVrdH3fkRRckp8rtodFcXCWtvJPMcRxqWY+gFUtI1u11qKR7TcPLbDK4wR6VeliSaNo5FDIwwynoRVew0y00yNo7GARKxy2CTk/jXVJVvaxcWuTW/fysZLk5HffoTT20N1H5dxEkqddrqCKeqCNQqABVGAAMYp1B6Vtyq9yLu1hjOu8KWAY9ATyafXPah4bkvfEcGpLdFFjKkpjnjsD710NY0qlScpKcbJPTXdd/IucYpLld/0CszxDqE2maLNdWyhpFwBuGQMnGa06p6tfQadpstxdoXiUAFQM7s8YoxH8Gfvcuj17efyCn8a0v5FTw1qU+q6OlzdKok3FSVGAcd616z9G1C21LTkns4zFHkrsIxtI+laFLC60Ie9zaLXv5hV+N6W8grFh8MW0OvvqqyyF2JYRnoGPU1tUZqqlCnVcXNX5Xdeoo1JQvyvcKKiurlLS0luJc7I1LNj0FZ+ia9BrkUrwI8ZiYBlf36GiVenGoqTfvPZAoScXNLRCeINb/sSxScQGYu+wDOAOM81d067F/p0F0EKCVA209qp67qlppdisl9CZkd9oTaDk9e9XbG5ivLGG4twRFIoKgjGBWEJt4qUfaX0Xu21Xnctx/dJ8vXczb7wzbX2tRajJLIrptJQdGx0raooJwCa3p0KdJylBWcnd+bIlOUklJ7BRWPpXiS01e+mtbdZFaIFssOGAOM/rWxTo1qdaPPTd0E4Sg7SVgrC8Q+I/wCw5YE+zGbzckndgAD+tbtMkhilx5savtORuXODU4iFWdNxpS5Zd7XHTlGMrzV0LG/mRq4BG4A4NOoorczCiiigAooooAKKKKAMfSf+Q1rH/XVP/Qa2Kx9J/wCQ1rH/AF1T/wBBrYoAKKKKACiiigAooooAKKKKACs2D/kab7/rytv/AEOetKs2D/kab7/rytv/AEOepe6Je6NKiiiqKCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAoNFBoAzdO/wCQtq//AF3j/wDRSVpVm6d/yFtX/wCu8f8A6KStKpjsTHYKKKKooKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArHtv+Ruvf+vaP+ZrYrHtv+Ruvf8Ar2j/AJmgC3rHl/2Pd+ezLH5LbinUDHasDwD5X9n3XlPIX8wbgwwBxxiukv50trGeeVDIkcZZlAzkY6VleFdStdQspvsdktmI3+ZF5Bz3zgV5VaMHmFKTkr2emt/8jrg5fV5K2l0btFc94wu72z0uOSwnEP7wBzuAJGOMZrW0x5pNMt3umV5mjBdlIIJx7V2RxEZV5ULO6Sd+mpi6bVNVL7mNqD60PFVuLWHdZZXc20EAfxZPY10eM1yGq6vfQeN7a0iugtuzRqY+xBPOfeuurlwVSM6lZRbdpdenp5GleLUYXS26fqZml+HbHSLmWezD75Bj5myFHoK574TDPgNP+vy5/wDRrVsaH4mj1u8mgS3aLyxuUls7hnH4Vzvw2v4NN+G/2q6YrGl5c5wMk/vm4p4ephVTU6FlBc3kulznrxqvERjP4rP9D0DFYHiHw1/blzBL9qMPljaRtzkZzx6GtTTdRt9Us1ubRiY2JHIwQR2NZXiRdcNza/2KSI8/PtI6++e1VjHRq4W8oucXbRdfuN6PPCrZPlfmbyJsRVBJ2jHNYdv4aFv4mk1X7UzByWEeO59/StxAwUb+uOfrTq6amHpVnFzXwu69TONSULqL3CiiitzMxvEur3Gjaek9rAJWZ9pLA4Ue+K0NPuXu9OguJY/KeRAzIe1WTg9aPpWEadRVnNz91rbt5mjlHkUba9zPuNcsLbUo7CabbPJjC4OBnpk1oVm3Gg2N1qkd/NETPHjB3cHHTIrSope25pe1ta+lu3n5hPksuX5hUVxbRXdu0NxGskbDDKw4NS1HPcRW0LTTyLHGoyzMcAVrLl5XzbEK99BtraQWUCw2saxRr0VRU1RW9zDdwLNbSLLG3RlOQalohy8q5NvIHe+oHpXOW82unxZJHNGf7PycHaNu3tg+tdHRWNai6ri1Jqzvp18n5Fwny30vca6LJGUkUMrDBUjIIqvZadaadGyWUCwqxywXvVqitXCLkpNaojmdrFe8sba/h8q8iWVM5w3Y1LFEkESxxKERRhVUYAFPop8kVLmtqF3awmaDyprntS0G/u/ElvfwXYSCPblcnK46gDvmuiHSsaVSc5SUoWSenn5lzjGKTTvf8DA0Pwwuj6lcXQuWl8xSqqVxgEg8+p4rfoop0MPTw8OSkrIKlSVSXNJ6hRRRW5mFFFFABRRRQAUUUUAFFFFAGPpP/Ia1j/rqn/oNbFY+k/8AIa1j/rqn/oNbFABRRRQAUUUUAFFFFABRRRQAVmwf8jTff9eVt/6HPWlWbB/yNN9/15W3/oc9S90S90aVFFFUUFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUVz8+tzxfEG30YmJbSTTXuiSPm3iRVHOemCaOtg6XOgorjNS8V3Frr2u2jXdtaWun2lpNHcPC0u1pZHVtwU5P3QBjpnNTWPxA0ya812O9ElnDo9ykDTSQyAS7lUjGVGSWbAUZJ4PQigDraDWdo+vadr0EsumTtIIX8uVJInieNsA4ZHAYcEHkVelljhjaSV1RFGWZjgAfWgChp3/IW1f/rvH/6KStKsrT5V/tDV5U+dfNRhs53DyU6etUvDPiaXxDdawkunzWI0+6ECpOMSMDGrbiATjO7j2qYkx2OiormNE8TXevajr1nBpz2UmmmNIBeDaZGdCwZgCcLnHviqFprHjOPxZY6VqcOgyxyq01x9jabfDEOA3zDHLYAHfn0NUUdtRXMeMvEN7oUOnRaWbBbm+uTCH1B2SGNRGzsxI56L+taXh241O70sTaxJpssrsSj6a7PEyYGDlu/WgNjVooooAKKKKACiiigAooooAKKKKACiiigAooooAKx7b/kbr3/r2j/ma2Kx7b/kbr3/AK9o/wCZoAt6xI0Wj3kkbBHWFiGI6HFc/wCAJpJNPuleQMqSDavcZHNdVLEk0bRyqGRxhlPcVBYada6bE0dlCsSsckDua8+phqk8ZCun7sU1b1OiNWKoyptauxg+OrWa50yD7PA8pSXJKAkqMelbOiRPBolpHJGYnWJQyHsav5orSGEjDEyxN9ZJK3oTKs3SVPscfq0Fy3jq0kTThJGCn73aSCM8sT0yK649KY1zCkywvKiyN91CwyfwqQmjD4eNGVSSlfmd/QKlRzUU1sjivBVjc22rXjXFo8K7NoLAjBz096yPBdhNqXwna3tVDSm9uGUE4ziZq6zQPEU+sX1xBNaeSsQyDk8c4wfeua8Aag+l/C5ruKLzWS8uML25mYZNeXhqeGjguSMm4Wnd2s/PQuvKq8bBtWl/wx1vhbTp9M0VILtVWUuzFQc4zUXiHxGdDuLeMWpm83knOMDPQe9XNB1J9W0uO6lh8pmJBHY4PUVneJ5tWjubQaXbCZM5Y+WH+bsOen1roqSVLL4vDNpJKzSu7adNC4rnxDVRLrfWx0KNuUHpkZxSeahlMYdS45K55H4ULkoNww2ORXPWmhxw+Lpb8agruSW8gH5hn156V31qlSDgoRvd2etrLv5+hhCMZXu7WOjJxTY5UlXMbK49VOaZeW/2uymt95TzEK7h1GRWX4c0F9Dt5Y5LjzjIwbgYC4qpTqqtGKjeLvd32+QlGLg23r2F8TX2oWOnpJpcXmSF8OQm7aPpWhp808+nwSXcflzMgLpjGDVmiiNKSrOpzOzW3ReYOacFG3zCikyKG+7xW5mLXznHceBH8HXs2tXF8dbW8uN7o12cYnbHI+T7uOle1aBd65cahdLq0OyFR8hKAYOeg9Rim+C9AuND8KLpuprE8n2i4kYKdylXlZh+jCuOE1i6LaTjdNa6PsbSTo1FeztZ+Rk/B03LfDPT3vpHlmdnbfIwYsu47ST3O3bk+ua7qvJ5NKs/D/hnQ9B8Sww3V7CLlbXTzdLHbOvmZErs2ANqsoHUgscA16DoOzTPDemW15qMdzIlukf2gyZExCjkEnn61vTgqVNR7fIzk3KRr0UdaK1JCiiigAoqrf6naaZAJr6XykJ2g4JyfoKmgnjuYEmgcPG4yrDuKhVIOTgnqunUrldua2hJRRRVkhRRRQAUUUUAFFFFABRRRQAUUUUAYGpeJjp+vw6d9kaRZNuXzzz6DvW+KY0MbSLIyKXX7rEcj8afXPShVjKTnK6b002XbzNJyi0uVW7+Zj6T/wAhrWP+uqf+g1sVj6T/AMhrWP8Arqn/AKDWxXQZhRRRQAUUUUAFFFFABRRVLWdWttC0a61O+3/Z7WMyOI13MQOwHc+1AF2s2D/kab7/AK8rb/0Oeq2heJU16SVE0nVrDy1DbtQszCHz/dJ60uqavpHh6/S61G7WKe/aCziiLDdId7Bdq9TzIcn0pNaolm1RWZrniLTPDlgbvV7pII8hVBYbnYkABR3OTWkDkZFMoWiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKQ0ALRWDpNrf32i2V3Nrl8JLi3jlYLHBgFlBOP3XvVz+y7z/oO6h/3xb//ABqpUrrYhSur2NKis3+y7z/oO6h/3xb/APxqj+y7z/oO6h/3xb//ABqnd9h3fY0q4nxB4OsvEnxGs7nXNGt9R02HS5Iw1zErokpkUgYPfANdJ/Zd5/0HdQ/74t//AI1R/Zd5/wBB3UP++Lf/AONUr63sHM7Wsed6n4DOmz+KLfwxoUVpZ3lpYiCO1jWNJJEmYvgDuBjNaN/ot+0/iiOXRLi9iudRtryAxTiJmVY4lLRvniRWQkA4Bx15rs/7LvP+g7qH/fFv/wDGqP7LvP8AoO6h/wB8W/8A8ap3fYV32MfwXFrCNqD6mt4to7p9k/tERfaiNvzbzHwRnGM89c9qz9R8OaomsT3+pwJ4psmkLxWksnltbL/dSM/upMerbT711H9l3n/Qd1D/AL4t/wD41R/Zd5/0HdQ/74t//jVF3vYd32GaSd2oaodhQGaP5SMFf3KcVR8O6fdWniHxLPcwtHFd3ySQMf41EEakj8QR+FXItCnhmmlj1vUA87BnOyDkhQo/5ZegFS/2Xef9B3UP++Lf/wCNUk2uhKulaxnaLYXVt4z8SXc0LJb3bWxgc4w+2LDY+hpPC9leC81jVNVgaK6vb11iV8Erbx/LEOOxwX/4Ga0v7LvP+g7qH/fFv/8AGqP7LvP+g7qH/fFv/wDGqd32Ku+xiaw8uow6fqF14Sa/S2uZd9vOI2uIcZVZY1LbTnGeoOD68VJ4H0m401dXuJLI6bb6hem4t7AlcwLsVTkKSqlmUsQCRz9a1/7LvP8AoO6h/wB8W/8A8ao/su8/6Duof98W/wD8aoTa6Bd9jSorN/su8/6Duof98W//AMao/su8/wCg7qH/AHxb/wDxqi77Bd9jSorN/su8/wCg7qH/AHxb/wDxqj+y7z/oO6h/3xb/APxqi77Bd9jSorFuIb2wu7Fv7WurhJbgRvHKkO0gqx/hjB6gd62hQncadwooopjCiiigAooooAKKKKACse2/5G69/wCvaP8Ama2Kx7b/AJG69/69o/5mgDYooooA57xfeX9jp0UunyiL95h2JAPTjGa1tLeeTS7d7sqZmjBcrjBP4VW13Q4tctY4pZWiMbblZRn9Ku2VoljZRW0ZJSJAoJ6mvPp06yxk6jvyNK2ul/TodEpQdGMVvf8ArU4jW2X/AIWHbcPxJEDz15HSu9AzUL2VtJcrcSQRtMn3ZCgLD8anHFGEwksPUqybvzyuFaqqkYpLZWGhFXJUAE8nA61xfwmAPgJQeR9suf8A0a1dsa4n4S/8iGn/AF+XP/o1q63/ABF6P9Dhl/Fj6P8AQ7UKFGFGB6ClxRRWpsIRWFbeGkt/Ej6r9pY7mZhHjoT7+lb1FYVcPTrOLmr8ruvUuNSULqL3Cio7ieO2t3mmO1I1LMfQCqek61aazE72Zb92cMrDBHpVurTjNU2/eeyEoSceZLQj13W49DtUmkiaUyPtVV47Zq5ZXa31jDcxgqsqhgD1FPubWC7j8u5hSVM52uoIzT0RY1CooVVGAAMACs4xre2cnL3LaK3X1KbhyJJanP6ho+p3PiW3vbe72WybdybyMY6jHQ5roqKKdKhClKUo3953YTqOaSfQMUUUVuZlS90rT9RKnULG2uigO0zwq+31xkcVlaz4Wg1T7KsMgtY7ddixxoNoX0A7V0FFY16FPEU3Tqq6ZcKkqcuaL1GouyNVBztAGTWLB4mgm8QvpQhkDqxXzOxI68VuVALK3W6NysEYnYYMgQbj+NTWjVbj7KVrPXTddhwcFfmV+xPRRRXQZnOeMrC61DSoks4DMyyhiB1AwelauiwSWuiWkM6CORIwGUdjV6iuSOFhHEyxCerSX3Gzqt01T6IKKKK6zEKKKrX9/BptlJdXRIjjxnAyeTiplKMIuUnZIaTbsizRVTTdSt9Vsxc2hJQkghhgg1bohOM4qUXdMJRcXZhRRRVCCiiigAooooAKKKKAMfSf+Q1rH/XVP/Qa2Kx9J/5DWsf9dU/9BrYoAKKKKACiiigAooooAK53x5BZ3Pg+6h1S5uLS0do/Nnt497RgOp3YIPAxyccDJroqQgMMHkHqDQBwOh61FY+Kb2K28RNqvh+HT/tNxd3Fwsq2su/p5o4wVydpPGO2aufEOG1n0TTLtY4pJBq1h5c20EhTcJ0Poa6xbO2SBoEgiWJs7owgCnPXIp7QRugR40ZVIIUrwCOn5U76ryDucp8TraCXwDfyyQxvJEYzG7KCU/ep0PautT7g+lJJEksZSVFdT1VhkGngYpAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUGig0AZvhz/kVtK/68of8A0AVpVm+HP+RW0r/ryh/9AFaVTHZEx+FBRRRVFBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAZur/AOu0z/r9X/0B60qzdX/12mf9fq/+gPWlUrdkrdhRRRVFBRRRQAUUUUAFFFFABWPbf8jde/8AXtH/ADNbFY9t/wAjde/9e0f8zQBsUUUUAFFFFABSE4paMUAc/oPiV9avbiB7QwiIZBznvjB96yfhL/yIaf8AX5c/+jWrs1iRCSiqpY5JAxmuM+Ev/Ihp/wBflz/6NauOjCrT5Y1Zcz11tbqia0oyrxcFZWf6HbUUUV2FBRRRQAyWJJomjlUMjDDKe4qvp+l2elxNHYwiJWOW5Jyfqat0VDpwclNrVdSuZpWvoFFFFWSFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABUN3aQXtq9vdRiSJxhlJ61NRSlFSVnsNNp3RXsrG30+1FvZxCONTkKDmrFFFKMYxSjFWSBtt3YUUUVQgooooAKKKKACiiigDH0n/AJDWsf8AXVP/AEGtisfSf+Q1rH/XVP8A0GtigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACg0UGgDN8Of8itpX/XlD/wCgCtKs3w5/yK2lf9eUP/oArSqY7ImPwoKKKKooKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDN1f8A12mf9fq/+gPWlWbq/wDrtM/6/V/9AetKpW7JW7CiiiqKCiiigAooooAKKKKACse2/wCRuvf+vaP+ZrYrHtv+Ruvf+vaP+ZoA2KKKKACiiigAooooADXE/CX/AJENP+vy5/8ARrV2xrifhL/yIaf9flz/AOjWrJ/xF6P9DGX8WPo/0O2ooorU2CiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDH0n/kNax/11T/ANBrYrH0n/kNax/11T/0GtigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACg0UGgDN8Of8AIraV/wBeUP8A6AK0qzfDn/IraV/15Q/+gCtKpjsiY/CgoooqigooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAM3V/9dpn/X6v/oD1pVm6v/rtM/6/V/8AQHrSqVuyVuwoooqigooooAKKKKACiiigArHtv+Ruvf8Ar2j/AJmtisGdryy8RXF1DYS3UcsKIDGQMEfWgDXvUnksZktH2TMhEbHs2OKy/DNpqlnZyrrErOzPlAz7yo780861eqCW0W5AHUl1qvZeJ5NStzPY6XNcRb2j3xyKRuVirDPsQR+FYSoRlWjWu7pNWvpr3RoqjUHC25d1rW7fRLdJblXfe21VQcmrlndR3tnFcwklJVDLmsa8uZdQh8q88PXEyA5AZ14NTRandwxrHFoVwqKMKoZQAKIxre2bbXJbRdbg3DkSS1Nqisf+2L7/AKAl1/32tH9sX3/QEuv++1rczNg1xPwl/wCRDT/r8uf/AEa1atx4qe1u7a1udMmjnu2KQRtIu6QgFjgewBNcd4Z8SN4C8MW2n+I9Oubaee4uJI0C72YGQtnCg9Aw/Os38afr+hlJP2kX5P8AQ9SorEh168nhSWLRbpkdQyncoyDT/wC2L7/oCXX/AH2taGpsUVj/ANsX3/QEuv8AvtaP7Yvv+gJdf99rQBsUVj/2xff9AS6/77Wj+2L7/oCXX/fa0AbFFY/9sX3/AEBLr/vtaP7Yvv8AoCXX/fa0AbFFY/8AbF9/0BLr/vtaP7Yvv+gJdf8Afa0AbFFY/wDbF9/0BLr/AL7Wj+2L7/oCXX/fa0AbFFY/9sX3/QEuv++1o/ti+/6Al1/32tAGxRWP/bF9/wBAS6/77Wj+2L7/AKAl1/32tAGxRWP/AGxff9AS6/77Wj+2L7/oCXX/AH2tAGxRWP8A2xff9AS6/wC+1o/ti+/6Al1/32tAGxRWP/bF9/0BLr/vtaP7Yvv+gJdf99rQBsUVj/2xff8AQEuv++1o/ti+/wCgJdf99rQBsUVj/wBsX3/QEuv++1o/ti+/6Al1/wB9rQBsUVj/ANsX3/QEuv8AvtaP7Yvv+gJdf99rQBsUVj/2xff9AS6/77Wj+2L7/oCXX/fa0AbFFY/9sX3/AEBLr/vtabLrl3DE0kuj3CIgLMzSKAAOpJoA2qK5TSfHlvrnm/2VZS3BhALgMAQD0bBxlTg4YcH1q1p/il9VsUvNO0ya5tpCQkscilWwSDg/UGgCzpP/ACGtY/66p/6DWxWPoiXJvNQubm2e38+RWVXIJ4GO1bFAGNp+s3N34n1PS57PyEs4opI5DIGMocuM4HQfJ061s1Ui06CHVbjUU3efcRRxPk8bULFeP+BmrdABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUGikJoAzvDn/IraV/15Q/8AoArSqhocMlv4d06CdCkkdrEjqeqkIARV+lHZEx+FBRRRTKCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAzdX/wBdpn/X6v8A6A9aVUNThklksDGhYR3Qd8dhtYZ/UVfFJbslbsKKKKZQUUUUAFFFFABRRRQAUUUUAMliSaNo5UV43GGVhkEehFYHgfT20vw69o1r9kVb67aOEJsCobiQrgdgQQR7GuioxigAooooAKKKKAPOtW0TxM/xG0vVntLG6t0vysMqzSbra38lwQV8vCkkkk55O0dBxa8Tot/qVrqP2TxJC8ME9vDJpylSH3j7y43YOwEMflI613WKMUdLB1uUtF+3f2JZf2tt+3eQn2jb08zaN361eoxRTerEtEFFFFIYUUUUAFFFFABRRRQAUVmaxrtrob2X25ZVivLhbZZlUFI3b7u854BPGfUig67bHxINEjEkl2Lf7RIUUFYkzhdxzwWOcD2NAGnRQKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKoa46poF+0lk9+gt5C1oi7mnG0/IB3J6fjV+ihgeYWtprXiG0uJra3ktdRmiit5oby1msoYrUEkwxOVJLEnBbHToFrpvhzZahp3gyC01SyhspIppwkMRbCoZXI4KjHXj2xXUYpQMU7gFFFFIAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAPOvFOsXF342uNGQ635FlYpOY9G+Vy7sw3s2RwoUYXPJJ4NdFYa0E+HaaxHef2l5NgZvtBj2GYqhOWXscjkdjml1bw7fT642q6HqiaddTW4trjzLbzlkQElSBuXDDc2DyOeQaXS/DB0nTrXSYLoSaTFavDPBLDl53Y5Ll88dWyMd+2KX2bf11HpzXOWsbnUtEXwrqtxq15fNrXyX0M8u6Ms8LSqY16JtK446g85PNQafqOpwaH4Z8Vy6reTS6zfQx3Vq8hMAinJCqsfRSuV5HJwc5rpNM8FzWlxpY1DVDe2ejqy2MHkbGGVKAyNuO8hSQMAdc0yy8DyWz6fazamZtI0u5+02ln5GHVhnYrSbvmVd3AwDwMnirur/P8ADsTrb+tzsKKKKkYUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAQXtwLOwuLlhkQxtIQO+BmvM/COuT3l1oOpahqGtwy6oMubtP9CumZC3lxrnMZH8JwNwU9c16jLGssLxyLuR1KsD3Brj7DwPc28ml299rH2vS9HlEtjbC2COCqlU3vuO7aCcYA980LfUHsX/GGmWM+mvqWp6nqthBYxNI39nX8lvuHXkIRuPHH1rL0drvwj4LiN7Pe6hql/MTbW13dPO4kf7kW5iThQMse2GNauo+Hr/WNGtbHU9TjlMd6lxO6W2wTRpJvEe3ccdFBPOcdOav6x4d0rxBHCms2SXQgYvFuJGwkYJBBHajoBifDi41K48P3g1u9a9vIdTuoXlOcfLIRhR2UY4HpXXVzfg/wZY+D7e9jscE3d1JOWCkbVZiVTknO0HGe9dJTYdwooopAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAUdZ0q21vR7rTr9N8FxGUbHUehB7EHkH1FePWa3i+EdF1K9vVkj1a8lOr3t5M8KOI1ZIUd05VPl6cAnr1r2+meVH5ezYuz+7jigZ5PFerH4aitr+9Fzpt1qzR28ovZYrVIxHko87Dc8e4NjHBOBnAqpo8/8AaX9n6RNfyNbr4mngVLe5kx9n+yu6oGJ3bDzjnkdK9jMMZj2FFKf3SOKBDGGyI1znOcU/6/In+vz/AMzyi9ubTTfGsrfbfttxHqEMUVql5LDeQIdi7EiOVki7k8ZGTkkV6yKb5UZk37F39N2OafS6WH1uFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAf//Z"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tree_Hitters.JPG](attachment:Tree_Hitters.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Odk2lcYM1XQe"
   },
   "source": [
    "\n",
    "## Ventajas\n",
    "* **No lineal:** Los árboles son una forma inteligente de representar las no linealidades\n",
    "\n",
    "* **Fácil de entender y comunicar:** La salida del árbol de decisiones es muy fácil de entender. No requiere ningún conocimiento estadístico.\n",
    "\n",
    "* **Útil en la exploración de datos:** es la forma más rápida de identificar variables significativas (las de los nodos de más arriba). Con los árboles de decisión podemos crear nuevas variables/ características que tienen mejor poder para predecir la variable objetivo.\n",
    "\n",
    "* **El tipo de datos no es una restricción:** Puede manejar variables tanto numéricas como categóricas\n",
    "\n",
    "## Desventajas\n",
    "\n",
    "* **Ajuste excesivo:** El ajuste excesivo es una de las dificultades más prácticas para el modelo de árbol de decisión. Este problema se resuelve estableciendo restricciones en los parámetros del modelo.\n",
    "\n",
    "* **Estructura lineal:** Cuando la estructura de los datos es lineal, el arbol no anda bien, porque las variables importan siempre y con cart solo algunas variables importan en cada nodo. \n",
    "\n",
    "* **Poco robusto:** Cambios en la base puede generar grandes cambios en el arbol resultante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRiltndx1XQe"
   },
   "source": [
    "## Ejercicio: Árbol de decisiones sobre datos del Titanic\n",
    "En este ejercicio veremos los árboles de decisión utilizando el conjunto de datos Titanic:\n",
    "1. Aprender a predecir a los sobrevivientes con árboles de decisión\n",
    "2. Exploración y procesamiento de conjuntos de datos\n",
    "3. Funciones relevantes para los árboles de decisión\n",
    "4. Impureza de Gini\n",
    "5. Encontrar la mejor profundidad de árbol con la ayuda de la validación cruzada\n",
    "6. Generación y visualización del modelo final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Variable|Definition|Key|\n",
    "| --- | --- | --- |\n",
    "|Survival|Survival|0 = No, 1 = Yes|\n",
    "|Pclass|Ticket class|1 = 1st, 2 = 2nd, 3 = 3rd|\n",
    "|Sex|Sex||\n",
    "|Age|Age in years||\n",
    "|Sibsp|| # of siblings / spouses aboard the Titanic\t\n",
    "|Parch|| # of parents / children aboard the Titanic\t\n",
    "|Ticket|Ticket number||\n",
    "|Fare|Passenger fare||\n",
    "|Cabin|Cabin number||\n",
    "|Embarked|Port of Embarkation|C = Cherbourg, Q = Queenstown, S = Southampton|\n",
    "\n",
    "\n",
    "Fuente: [Kaggle](https://www.kaggle.com/c/titanic/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTA9rxMv1XQf"
   },
   "source": [
    "### Limpieza de la base de datos del Titanic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XH_nx_Cn1XQf"
   },
   "outputs": [],
   "source": [
    "# (Si aún no instalaron alguna librería recuerden primero hacer ese paso)\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wm2HIWmL1XQg",
    "outputId": "b76392a0-1027-43d0-b9c1-3c33e1454a73"
   },
   "outputs": [],
   "source": [
    "# Cargamos los datos \n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Guardamos el ID de los pasajeros del grupo a predecir \n",
    "passenger_id = test['PassengerId']\n",
    "\n",
    "original_train = train.copy()\n",
    "\n",
    "# Info del df\n",
    "print(train.info())\n",
    "#print(test.info())\n",
    "# Miramos las primeras filas del df\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación queremos generar una función que nos extraiga el título de la persona. Por ejemplo, el título de `Braund, Mr. Owen Harris` sería `Mr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S_ntRUTs1XQi"
   },
   "outputs": [],
   "source": [
    "# Definir función para extraer los 'titles' de los nombres de los pasajeros\n",
    "def get_title(name):\n",
    "    '''\n",
    "    input: nombres de los pasajeros, incluyendo el title\n",
    "    return: title\n",
    "    '''\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función anterior usa algo que se llaman expresiones regulares para buscar texto. El uso de expresiones regulares es algo que solía ser bastante complicado y requerir bastante estudio, pero ahora, con la ayuda de los LLMs, la cosa se hizo mucho más fácil. Por ejemplo: Si están usando VSCode usen _Generate_ de copilot con un prompt que diga algo como \"create a function, called get_title, that extracts the word right before the first dot in a string\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hsgF2qJ1XQi",
    "outputId": "75b66734-59e3-421d-8db8-5f635def55ae"
   },
   "outputs": [],
   "source": [
    "# Ejemplo\n",
    "get_title(train.Name[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.Name[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tarea para la casa: hacer un heatmap de NaN en train y test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a crear nuevas columnas con el dataset que contengan información relevante (lo pueden ver en detalle en sus casas).\n",
    "\n",
    "Vamos a hacer varias cosas:\n",
    "* Calcular el tamaño de la familia.\n",
    "* Crear una nueva columna llamada `Has_cabin` que tiene un 0 si `Cabin` es `NaN` y un 1 si no.\n",
    "* Completar los _missing values_ de `Embarked` con `S` (el valor más común).\n",
    "* Completar los _missing values_ de `Fare` con su mediana.\n",
    "* Completar los _missing values_ de `Age` con un valor aleatorio entre la media +/- 1 SD.\n",
    "* Obtener el título y reemplazar los raros (como `Countess`) por `Rare`.\n",
    "* Convertir la variable `Age` en numérica.\n",
    "* Generar dummies para `Title` y `Embarked`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKly44mb1XQj"
   },
   "outputs": [],
   "source": [
    "dummies_train_df = []\n",
    "dummies_test_df = []\n",
    "\n",
    "for dataset in [train, test]:\n",
    "    # Tamaño de la familia\n",
    "    dataset['Family_Size'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "    # Viaja solo?\n",
    "    dataset['Alone'] = 0\n",
    "    dataset.loc[dataset['Family_Size'] == 1, 'Alone'] = 1\n",
    "    # Tiene cabina?\n",
    "    dataset['Has_Cabin'] = dataset[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "    # Reemplazamos los missings en Embarked con la opción más común #train.groupby(['Embarked']).count()\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "    # Reemplazamos los missings en Fare con la mediana\n",
    "    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n",
    "    \n",
    "    # Imputamos los nulls en la edad con valores aleatorios entre la media +- 1 SD\n",
    "    age_avg = dataset['Age'].mean()\n",
    "    age_std = dataset['Age'].std()\n",
    "    age_null_count = dataset['Age'].isnull().sum()\n",
    "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
    "    dataset.loc[np.isnan(dataset['Age']), 'Age'] = age_null_random_list\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "    \n",
    "    # Nueva columna con titles\n",
    "    dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "    # Agrupamos los títulos menos comunes\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'],\n",
    "                                                'Rare')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Mr') \n",
    "   \n",
    "    # Convertimos Sex en numerica\n",
    "    dataset['Male'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "    \n",
    "    if dataset is train:\n",
    "        # Convertimos Title en dummies  \n",
    "        titles_train = pd.get_dummies(train['Title'], drop_first=True)\n",
    "        dummies_train_df.append(titles_train)\n",
    "        # Convertimos Embarked en dummies\n",
    "        embarked_train = pd.get_dummies(train['Embarked'], drop_first=True)\n",
    "        dummies_train_df.append(embarked_train)\n",
    "    else:\n",
    "        # Convertimos Title en dummies    \n",
    "        titles_test = pd.get_dummies(test['Title'], drop_first=True)\n",
    "        dummies_test_df.append(titles_test)\n",
    "        # Convertimos Embarked en dummies        \n",
    "        embarked_test = pd.get_dummies(test['Embarked'], drop_first=True)\n",
    "        dummies_test_df.append(embarked_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos variables irrelevantes y unimos con las tablas de dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-gBn1DN1XQj",
    "outputId": "889428c8-c8cf-410f-90e2-985b1a99f6b2"
   },
   "outputs": [],
   "source": [
    "# Eliminamos variables originales que no nos sirven y concatenamos las dummies creadas\n",
    "drop_elements = ['PassengerId', 'Name', 'Sex', 'Ticket', 'Cabin', 'Parch', 'SibSp', 'Embarked', 'Title']\n",
    "\n",
    "train = pd.concat([train, dummies_train_df[0], dummies_train_df[1]], axis=1)\n",
    "train = train.drop(drop_elements, axis = 1)\n",
    "train.columns = train.columns.str.lower()\n",
    "\n",
    "test = pd.concat([test, dummies_test_df[0], dummies_test_df[1]], axis=1)\n",
    "test = test.drop(drop_elements, axis = 1)\n",
    "test.columns = test.columns.str.lower()\n",
    "\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-OVDbsIG1XQk",
    "outputId": "a7e9d0dd-9505-4186-dc65-c9a8ba9f5ef3"
   },
   "outputs": [],
   "source": [
    "print(train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos que no hayan quedado missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqOxu4c11XQk"
   },
   "outputs": [],
   "source": [
    "# Probamos que no hayan quedado missing values:\n",
    "columns_with_nan = train.columns[train.isna().any()].tolist()\n",
    "print(\"Columns with NaN values:\", columns_with_nan)\n",
    "columns_with_nan = test.columns[test.isna().any()].tolist()\n",
    "print(\"Columns with NaN values:\", columns_with_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDv88h9u1XQk"
   },
   "source": [
    "## Visualización de la base procesada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARV4arne1XQl"
   },
   "source": [
    "### Heatmap\n",
    "Este mapa de calor es muy útil como observación inicial porque permite hacerse una idea del valor predictivo de cada feature. El valor del mapa de calor se sitúa entre (-1,0) y (0,1). A medida que aumenta el valor (en términos absolutos), aumenta la correlación entre los atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ceT7kUuH1XQl",
    "outputId": "a27dfbe6-4706-4e37-a24f-747f642a80d5"
   },
   "outputs": [],
   "source": [
    "colormap = plt.cm.viridis\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Correlacion de Pearson entre las Xs', y=1.05, size=15)\n",
    "sns.heatmap(train.astype(float).corr(),\n",
    "            linewidths=0.1,\n",
    "            vmax=1.0, \n",
    "            square=True, \n",
    "            cmap=colormap, \n",
    "            linecolor='white', \n",
    "            annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdxLnfSH1XQl"
   },
   "source": [
    "### ¿Cómo decide un árbol dónde dividirse?\n",
    "Los árboles de decisión utilizan distintos algoritmos para decidir cómo dividir un nodo en dos o más subnodos. La creación de subnodos aumenta la homogeneidad de los subnodos resultantes. En otras palabras, podemos decir que la pureza del nodo aumenta con respecto a la variable objetivo. El árbol divide los nodos en todas las variables disponibles y luego selecciona la división que da como resultado los subnodos más homogéneos.\n",
    "\n",
    "\n",
    "#### Índice de impureza de Gini\n",
    "El Gini da una medida de la impureza de un nodo. Se calcula como:\n",
    "\n",
    "$Gini =  \\sum \\limits _{k=1} ^{K} \\hat{p}_{mk} (1-\\hat{p}_{mk}) $\n",
    "\n",
    "donde $\\hat{p}_{mk}$ es la proporción de observaciones que en el dataset de entrenamiento en la región m de la clase k. \n",
    "\n",
    "Por la forma en la que está construido este índice, su valor será menor cuando $\\hat{p}_{mk}$ se acerque a 1 o a 0 (es decir, cuando las observaciones del nodo son más parecidas). Cuanto menor sea el valor de Gini, menor será la desigualdad. Los árboles de decisión intentarán encontrar la división que reduzca más la Impureza de Gini en los dos nodos resultantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnKoBwr_1XQl"
   },
   "outputs": [],
   "source": [
    "# Definimos la función de la impureza de Gini \n",
    "def get_gini_impurity(survived_count, total_count):\n",
    "    survival_prob = survived_count/total_count\n",
    "    not_survival_prob = (1 - survival_prob)\n",
    "    random_observation_survived_prob = survival_prob\n",
    "    random_observation_not_survived_prob = (1 - random_observation_survived_prob)\n",
    "    mislabelling_survived_prob = not_survival_prob * random_observation_survived_prob\n",
    "    mislabelling_not_survived_prob = survival_prob * random_observation_not_survived_prob\n",
    "    gini_impurity = mislabelling_survived_prob + mislabelling_not_survived_prob\n",
    "    return gini_impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6pOu0tcG1XQl",
    "outputId": "c6eda6a3-ea4d-4dc0-b39b-3ce85494a1b5"
   },
   "outputs": [],
   "source": [
    "train.survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7gIGunw1XQm",
    "outputId": "a017876e-77aa-4b1c-a20d-a45a08b74579"
   },
   "outputs": [],
   "source": [
    "# Gini Impurity en nodo inicial\n",
    "survived = train.survived.value_counts()[1]\n",
    "total = len(train)\n",
    "gini_impurity_starting_node = get_gini_impurity(survived, total)\n",
    "gini_impurity_starting_node.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mano:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = (549+342)\n",
    "suma_k0 = 549/total * (1 - 549/total)\n",
    "suma_k1 = 342/total * (1 - 342/total)\n",
    "gini_a_mano = suma_k0 + suma_k1\n",
    "print(np.round(gini_a_mano,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos el promedio de los sobrevivientes entre hombres y mujeres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).agg(['mean', 'count', 'sum']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gini Impurity si hicieramos la partición según sexo\n",
    "gini_impurity_starting_node_male = get_gini_impurity(109, 577)\n",
    "gini_impurity_starting_node_female = get_gini_impurity(233, 314)\n",
    "print(np.round([gini_impurity_starting_node_male, gini_impurity_starting_node_female],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árbol de decisión de clasificación\n",
    "\n",
    "Para estimar el modelo vamos a usar [DecisionTreeClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) de Scikit Learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['survived'], axis=1)\n",
    "y = train[\"survived\"]  \n",
    "\n",
    "# Primero, probemos un arbol sin tocar ningun parametro (osea usando la configuración por default)\n",
    "decision_tree = DecisionTreeClassifier().fit(X, y)\n",
    "\n",
    "# Gráfico\n",
    "fig = plt.figure(figsize=(35,28))\n",
    "graph = plot_tree(decision_tree,\n",
    "                  impurity = True,   #mostrar impurity\n",
    "                  feature_names = list(train.drop(['survived'], axis=1)),\n",
    "                  class_names = ['Died', 'Survived'],\n",
    "                  rounded = True, \n",
    "                  filled = True   #colorear los nodos p/ indicar la clase mayoritaria (p/ clasif.)\n",
    "                 )\n",
    "fig.savefig(\"Tree_titanic_default.pdf\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cómo luce este arbol default? es facil de interpretar?\n",
    "\n",
    "**Interpretación de un arbol:** En cada nodo tenemos:\n",
    "1. splitting condition\n",
    "2. Gini impurity of the node\n",
    "3. Samples: cantidad de observaciones en el nodo\n",
    "4. Value: Cantidad de observaciones clasificadas como 'Died' y 'Survived'\n",
    "5. Class: etiqueta de cada nodo, lo que clasificamos en c/u. \n",
    "El color representa la clase y la saturación aumenta a medida que el Gini es menor (mayor % de observaciones clasificadas igual).\n",
    "\n",
    "Recordar: en un árbol de clasificación predecimos que una observación pertenece a la clase más recurrente dentro del nodo terminal al que pertenece (por ej., si para una observación determinada, la mayor parte de las observaciones del nodo al que pertenece son y=sobrevivió, para esa observación -y todas las de ese nodo- nuestra predicción será 'Sobrevivió')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Opcion `max_depth`\n",
    "La opcion `max_depth` nos indica cuántos niveles (o \"pisos\") tiene el árbol. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies = []\n",
    "\n",
    "depth_range = range(1, 16)  # Esto es para probar con distintas \"profundidad de un arbol\"\n",
    "\n",
    "for depth in depth_range:\n",
    "    tree_model = DecisionTreeClassifier(max_depth=depth)\n",
    "    tree_model.fit(X, y)\n",
    "    \n",
    "    train_accuracy = tree_model.score(X, y) #el atributo score nos da el mean accuracy of `self.predict(X)` w.r.t. y.\n",
    "    \n",
    "    train_accuracies.append(train_accuracy)\n",
    "# vemos el resultado\n",
    "np.round(train_accuracies,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos el desempeño para cada tamaño del árbol\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(depth_range, train_accuracies, label='Train Accuracy', marker='o')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train Accuracy vs Max Depth')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo parece indicar que cuanto más profundidad tiene el árbol, mejor. Miremos el árbol más grande que ajustamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un arbol con max_depth = 15\n",
    "decision_tree = DecisionTreeClassifier(max_depth = 15).fit(X, y)\n",
    "\n",
    "# Gráfico\n",
    "fig = plt.figure(figsize=(35,28))\n",
    "graph = plot_tree(decision_tree,\n",
    "                  max_depth = 15,\n",
    "                  impurity = True,   #mostrar impurity\n",
    "                  feature_names = list(train.drop(['survived'], axis=1)),\n",
    "                  class_names = ['Died', 'Survived'],\n",
    "                  rounded = True, \n",
    "                  filled = True   #colorear los nodos p/ indicar la clase mayoritaria (p/ clasif.)\n",
    "                 )\n",
    "fig.savefig(\"Tree_titanic_15.pdf\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué creen que puede estar pasando?\n",
    "\n",
    "Pues sí, estamos sobreajustando.\n",
    "\n",
    "Para sacarnos de encima este problema usaremos CV para elegir el hiperparámetro de la profundidad del árbol (`max_depth`). Este hiperparámetro determina el número máximo de atributos que consideran para cada predicción (siendo el máximo la cantidad de características Xs disponibles en el dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ORJ8JUHv1XQm"
   },
   "outputs": [],
   "source": [
    "X = train.drop(['survived'], axis=1)\n",
    "y = train[\"survived\"]       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos k-fold cross-validation para encontrar el valor del hiperparámetro `max_depth`. Hagamos un loop sobre los _splits_ a mano:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWk9rrB71XQm",
    "outputId": "d53e3ae3-aa12-48d1-d46e-e3fcc97a8a6d"
   },
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, shuffle = True, random_state=1)            \n",
    "accuracies = []\n",
    "max_attributes = len(test.columns)\n",
    "depth_range = range(1, max_attributes + 1) # Recuerdan por qué +1?\n",
    "\n",
    "for depth in depth_range:\n",
    "    fold_accuracy = []\n",
    "    tree_model = DecisionTreeClassifier(max_depth = depth)\n",
    "    \n",
    "    for i, (train_index, valid_index) in enumerate(cv.split(X)):   \n",
    "        x_train, x_valid = X.loc[train_index], X.loc[valid_index] \n",
    "        y_train, y_valid = y.loc[train_index], y.loc[valid_index] \n",
    "    \n",
    "        model = tree_model.fit(x_train, y_train) \n",
    "        valid_acc = model.score(x_valid, y_valid)\n",
    "        fold_accuracy.append(valid_acc)\n",
    "\n",
    "    avg = sum(fold_accuracy)/len(fold_accuracy)\n",
    "    accuracies.append(avg)\n",
    "    \n",
    "df = pd.DataFrame({\"Max Depth\": depth_range, \"Average Accuracy\": accuracies})\n",
    "df = df[[\"Max Depth\", \"Average Accuracy\"]]\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_index = np.argmax(df['Average Accuracy'])\n",
    "print(\"La mayor accuracy de cross validation es para max_depth \" + str(max_index + 1) + \", y vale \" + str(df['Average Accuracy'][max_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos concluir que el hiperparámetro óptimo es max_depth=4. Estimamos el modelo final con ese hiperparámetro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos el mismo gridsearch pero con `GridSearchCV`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_para = {'max_depth':depth_range, 'criterion':['gini']}\n",
    "tree_cv = GridSearchCV(DecisionTreeClassifier(), tree_para, cv=5, scoring='accuracy')\n",
    "tree_cv.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Parameters:\", tree_cv.best_params_)\n",
    "print(\"Best Accuracy Score:\", tree_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da diferente el valor de _accuracy_ de CV. Podemos cambiar el valor de `random_state` más arriba y ver cómo cambia justamente el valor de la _accuracy_.\n",
    "\n",
    "Este tipo de selección del hiperparámetro que hace un árbol más chico puede considerarse una técnica de pre-prunning (diferente al prunning tradicional que aparece en el Tibshirani)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que ya tenemos el valor del hiperparámetro ajustemos un árbol con todos los datos de _train_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etxgDCd71XQm",
    "outputId": "7d16f74f-b224-4029-b7e9-7f574736a172",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creamos un arbol con max_depth = 4\n",
    "decision_tree = DecisionTreeClassifier(max_depth = 4).fit(x_train, y_train)\n",
    "\n",
    "# Gráfico\n",
    "fig = plt.figure(figsize=(35,28))\n",
    "graph = plot_tree(decision_tree,\n",
    "                  max_depth = 4,\n",
    "                  impurity = True,   #mostrar impurity\n",
    "                  feature_names = list(train.drop(['survived'], axis=1)),\n",
    "                  class_names = ['Died', 'Survived'],\n",
    "                  rounded = True, \n",
    "                  filled = True   #colorear los nodos p/ indicar la clase mayoritaria (p/ clasif.)\n",
    "                 )\n",
    "fig.savefig(\"Tree_titanic_cv_max_depth.pdf\",bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etxgDCd71XQm",
    "outputId": "7d16f74f-b224-4029-b7e9-7f574736a172",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creamos un arbol con max_depth = 3\n",
    "decision_tree = DecisionTreeClassifier(max_depth = 3).fit(x_train, y_train)\n",
    "\n",
    "# Gráfico\n",
    "fig = plt.figure(figsize=(35,28))\n",
    "graph = plot_tree(decision_tree,\n",
    "                  max_depth = 4,\n",
    "                  impurity = True,   #mostrar impurity\n",
    "                  feature_names = list(train.drop(['survived'], axis=1)),\n",
    "                  class_names = ['Died', 'Survived'],\n",
    "                  rounded = True, \n",
    "                  filled = True   #colorear los nodos p/ indicar la clase mayoritaria (p/ clasif.)\n",
    "                 )\n",
    "fig.savefig(\"Tree_titanic_max_depth_3.pdf\",bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recordemos que no es lo mismo el accuracy *dentro* versus *afuera de la muestra* entonces probemos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dentro de la muestra\n",
    "acc_decision_tree = decision_tree.score(x_train, y_train)\n",
    "print(\"Accuracy train\",np.round(acc_decision_tree,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer la predicción sobre los datos de *testeo*. \n",
    "\n",
    "Acá *deberíamos* calcular la precisión del modelo final con la base de test (afuera de la muestra). Pero como estamos trabajando con la base de datos de la competencia, la **$y_{test}$ no la hicieron pública**. El **resultado final** a entregar en la competencia era la predicción de sobrevivientes sobre los ID de los pasajeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos de test\n",
    "test = test.dropna()  \n",
    "X_test = test\n",
    "\n",
    "# Predicción sobre los datos de test\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "submission = pd.DataFrame({\"passengerId\": passenger_id, \"survived_hat\": y_pred})\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tarea para la casa: armar la curva ROC para el arbol con tamaño max_depth =4 (elegido por Cross-validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Opcion `min_samples_leaf`\n",
    "La opcion `min_samples_leaf` nos indica cuántas observaciones quedan en el nodo terminal y es otra forma de controlar el tamaño de un arbol.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora, probemos un arbol con 10 observaciones en los nodos terminales como minimo\n",
    "decision_tree = DecisionTreeClassifier(min_samples_leaf=10).fit(X, y)\n",
    "\n",
    "# Gráfico\n",
    "fig = plt.figure(figsize=(35,28))\n",
    "graph = plot_tree(decision_tree,\n",
    "                  impurity = True,   #mostrar impurity\n",
    "                  feature_names = list(train.drop(['survived'], axis=1)),\n",
    "                  class_names = ['Died', 'Survived'],\n",
    "                  rounded = True, \n",
    "                  filled = True   #colorear los nodos p/ indicar la clase mayoritaria (p/ clasif.)\n",
    "                 )\n",
    "fig.savefig(\"Tree_titanic_10obs.pdf\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#repetimos la busqueda por Cross Validation del tamaño del arbol usando min_samples_leaf\n",
    "\n",
    "depth_range = [5, 10, 20, 30, 40, 50]\n",
    "\n",
    "tree_para = {'min_samples_leaf':depth_range, 'criterion':['gini']}\n",
    "tree_cv = GridSearchCV(DecisionTreeClassifier(), tree_para, cv=5, scoring='accuracy')\n",
    "tree_cv.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Parameters:\", tree_cv.best_params_)\n",
    "print(\"Best Accuracy Score:\", tree_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un arbol con 'min_samples_leaf': 30\n",
    "decision_tree = DecisionTreeClassifier(min_samples_leaf = 30).fit(x_train, y_train)\n",
    "\n",
    "# Gráfico\n",
    "fig = plt.figure(figsize=(35,28))\n",
    "graph = plot_tree(decision_tree,\n",
    "                  impurity = True,   #mostrar impurity\n",
    "                  feature_names = list(train.drop(['survived'], axis=1)),\n",
    "                  class_names = ['Died', 'Survived'],\n",
    "                  rounded = True, \n",
    "                  filled = True   #colorear los nodos p/ indicar la clase mayoritaria (p/ clasif.)\n",
    "                 )\n",
    "fig.savefig(\"Tree_titanic_min_obs_cv_20.pdf\",bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Árboles de regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso vamos a usar un árbol de regresión para predecir el salario de los jugadores de la MLB utilizando la, ya conocida, base de datos `Hitters`.\n",
    "\n",
    "En este caso no se minimiza el Gini sino en MSE pesado. Es decir, si definimos MSE como:\n",
    "\n",
    "\n",
    "$\\text{MSE}(N) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2$\n",
    "\n",
    "Cuando se propone una división, se calcula el MSE ponderado de los nodos izquierdo y derecho resultantes, con el objetivo de minimizar este MSE ponderado. La fórmula para el MSE ponderado de una división es:\n",
    "\n",
    "$\\text{MSE}_{\\text{split}} = \\frac{n_L}{n} \\cdot \\text{MSE}(N_L) + \\frac{n_R}{n} \\cdot \\text{MSE}(N_R)$\n",
    "\n",
    "Donde $\\text{MSE}(N_L)$ y $\\text{MSE}(N_L)$ son los valores MSE de los nodos  izquierdo y derecho, calculados de forma similar al MSE de un único nodo. $n=n_L+n_R$ es la cantidad de observaciones del nodo padre que se divide en un nodo de $n_L$ observaciones y uno de $n_R$ observaciones.\n",
    "\n",
    "Carguemos los datos y las librerías necesarias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ISLP import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hitters = load_data('Hitters')\n",
    "Hitters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos los datos, los acomodamos y los separamos entre train y test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos sacamos de encima los missings values\n",
    "Hitters.dropna(inplace=True)\n",
    "\n",
    "# Separamos en X e y\n",
    "X = Hitters.drop(['Salary'], axis=1)\n",
    "y = Hitters['Salary']\n",
    "\n",
    "# Convertimos las variables categóricas en dummies (one hot encoding)\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Separamos en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Miremos si está todo bien\n",
    "X_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos la correlaciones dentro de la base de entrenamiento\n",
    "colormap = plt.cm.viridis\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Correlacion de Pearson entre las Xs', y=1.05, size=15)\n",
    "sns.heatmap(X_train.astype(float).corr(),\n",
    "            square=True, \n",
    "            cmap=colormap, \n",
    "            linecolor='white', \n",
    "            annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:** Cuáles variables muestran alta correlacion lineal?\n",
    "\n",
    "Ahora inicializamos el árbol de regresión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_regressor = DecisionTreeRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hagamos algo parecido a lo que hicimos antes pero con el RMSE. Veamos cómo varían los errores de train y test a medida que aumentamos la complejidad del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distinguimos la performance adentro y afuera de la muestra\n",
    "train_RMSE = []\n",
    "test_RMSE = []\n",
    "\n",
    "depth_range = range(1, 21)  # You can adjust the range as needed\n",
    "\n",
    "for depth in depth_range:\n",
    "    tree_model = DecisionTreeRegressor(max_depth=depth)\n",
    "    tree_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_train = tree_model.predict(X_train)\n",
    "    y_pred_test = tree_model.predict(X_test)\n",
    "\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    \n",
    "    train_RMSE.append(train_rmse)\n",
    "    test_RMSE.append(test_rmse)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the accuracies\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(depth_range, train_RMSE, label='Train RMSE', marker='o')\n",
    "plt.plot(depth_range, test_RMSE, label='Test RMSE', marker='o')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Train RMSE vs Max Depth')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De nuevo es una buena idea hacer cross-validation.\n",
    "\n",
    "En este caso además de buscar el mejor valor de `max_depth`, vamos a hacer una grilla de búsqueda para otros hiperparámetros: \n",
    "- `min_samples_split`: minimo numero de particiones (splits) \n",
    "- `min_samples_leaf`: mínimo numero de nodos terminales (hojas/leaf) \n",
    "- `ccp_alpha`: costo minimo de complejidad para podar un arbol (en las slides 25 el $\\alpha$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos un rango de valores para los hiperparámetros\n",
    "param_grid = {\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 10, 20],\n",
    "    'min_samples_leaf': [ 5, 10, 20],\n",
    "    'ccp_alpha': [0.0, 0.01, 0.1, 1.0]  # Cost Complexity Pruning parameter for post-pruning\n",
    "}\n",
    "\n",
    "# Usamos GridSearchCV para encontrar el mejor modelo\n",
    "grid_search = GridSearchCV(tree_regressor, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos el mejor modelo\n",
    "best_tree = grid_search.best_estimator_\n",
    "best_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos predicciones para evaluar el modelo\n",
    "y_pred_train = best_tree.predict(X_train)\n",
    "y_pred_test = best_tree.predict(X_test)\n",
    "\n",
    "train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Mejores parámetros para el árbol de regresión: {grid_search.best_params_}\")\n",
    "print(f\"MSE de train: {train_mse:.4f}\")\n",
    "print(f\"MSE de test: {test_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos cómo se ve este árbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico\n",
    "fig = plt.figure(figsize=(35,28))\n",
    "graph = plot_tree(best_tree,\n",
    "                  feature_names = list(X_train),\n",
    "                  rounded = True, \n",
    "                  filled = True   #colorear los nodos p/ indicar la clase mayoritaria (p/ clasif.)\n",
    "                 )\n",
    "fig.savefig(\"Hitters.pdf\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico de un arbol de dos niveles\n",
    "prueba_tree=DecisionTreeRegressor(max_depth=2)\n",
    "prueba_tree_predic= prueba_tree.fit(X_train, y_train)\n",
    "\n",
    "fig = plt.figure(figsize=(35,28))\n",
    "graph = plot_tree(prueba_tree,\n",
    "                  feature_names = list(X_train),\n",
    "                  rounded = True, \n",
    "                  filled = True   #colorear los nodos p/ indicar la clase mayoritaria (p/ clasif.)\n",
    "                 )\n",
    "fig.savefig(\"Hitters_prueba.pdf\",bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tarea para la casa: probar hacer la prediccion para la variable log(salarios) -> chequear si es similar al libro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal vez les parezca un modelo simple, pero tengan en cuenta que con Ridge el MSE de test era Error 116135 y con Lasso 115610. La clase que viene vamos a ver cómo mejorar notablemente la performance de este modelo."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "deepnote_notebook_id": "d5ae7e63-e621-4577-a068-a3fc5977a204",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
