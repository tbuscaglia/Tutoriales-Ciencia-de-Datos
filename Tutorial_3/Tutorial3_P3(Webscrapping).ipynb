{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d2ad666-70bd-4034-9445-ab4acd916b36",
   "metadata": {},
   "source": [
    "# Tutorial de Ciencias de Datos (UdeSA) 2025\n",
    "## Tutorial 3 - Parte 3\n",
    "\n",
    "El objetivo de esta clase es ver cómo extraer datos de internet por medio de Web Scraping y cómo interactuar con una APIs. También veremos una introducción a los métodos de sentiment analysis.\n",
    "\n",
    "- Web Scraping\n",
    "- APIs\n",
    "- Sentiment analysis\n",
    "\n",
    "### Web Scraping: extrayendo datos de internet\n",
    "\n",
    "#### ¿Qué es web scraping?\n",
    "\n",
    "La práctica de recopilar datos a través de cualquier medio que no sea un programa que interactúa con una API o un humano que usa un navegador web. En general esto se logra mediante un programa automatizado que consulta un servidor web, solicita datos (generalmente en forma de HTML y otros archivos que componen las páginas web) y luego analiza esos datos para extraer la información necesaria.\n",
    "\n",
    "Fuente: Ryan Mitchell (2015). Web Scraping with Python.\n",
    "\n",
    "#### Antes de empezar\n",
    "\n",
    "###### Aspectos éticos y legales del web scraping\n",
    "Web scraping es la extracción de datos de sitios web, es una forma automática de guardar información que se presenta en nuestro navegador (muy usada tanto en la industria como en la academia). Sus aspectos legales dependerán de cada sitio. Respecto a la ética es importante que nos detengamos a pensar si estamos o no generando algun perjuicio.\n",
    "\n",
    "###### No reinventar la rueda\n",
    "Emprender un proyecto de web scraping a veces es rápido y sencillo, pero en general requiere tiempo y esfuerzo. Siempre es aconsejable asegurarse de que valga la pena y antes iniciar hacerse algunas preguntas:\n",
    "* ¿La informacion que necesito ya se encuentra disponible? (ej: APIs)\n",
    "* ¿Vale la pena automatizarlo o es algo que lleva poco trabajo a mano?\n",
    "\n",
    "#### Conceptos básicos sobre la web\n",
    "\n",
    "HTML, CSS y JavaScript son los tres lenguajes principales con los que está hecho la parte de la web que vemos (*front-end*).\n",
    "\n",
    "Una analogía para entender cómo funcionan:\n",
    "- HTML como la estructura de la casa.\n",
    "- CSS como la decoración interior y exterior.\n",
    "- JavaScript como el sistema eléctrico, del agua y otras funcionalidades que hacen una casa habitable\n",
    "\n",
    "Si quieren ver más cómo se unen HTML+CSS+Javascript: https://codepen.io/voubina/pen/gOZGPYx\n",
    "\n",
    "Fuente de las imágenes:\n",
    "https://geekflare.com/es/css-formatting-optimization-tools/ <br>\n",
    "https://www.nicepng.com/ourpic/u2q8i1o0e6q8r5t4_html5-css3-js-html-css-javascript/\n",
    "\n",
    "Fuente de la información: Instituto Humai - Curso de Automatización\n",
    "\n",
    "##### HTML\n",
    "\n",
    "- HTML quiere decir: lenguaje de marcado de hipertexto o HyperText Markup Language por sus siglas en inglés.\n",
    "- El código  HTML da estructura a los sitios web.\n",
    "- El código HTML se conforma por distintos elementos que le dicen al navegador cómo mostrar el contenido.\n",
    "- Esos elementos son etiquetas. Hay etiquetas para indicar qué contenido es un título, un párrafo, un enlace, una imagen, etc.\n",
    "\n",
    "|Etiqueta (Tag)     |Descripción|\n",
    "|:--------|:--------|\n",
    "|`<!DOCTYPE>`  | \tDefine el tipo de documento|\n",
    "|`<html>`      |\tDefine un documento HTML |\n",
    "|`<head>`      |\tContiene metadata/información del documento|\n",
    "|`<title>`     |\tDefine el título del documento|\n",
    "|`<body>`      |\tDefine el cuerpo del documento|\n",
    "|`<h1>` a `<h6>`|\tDefine títulos |\n",
    "|`<p>`         |\tDefine un párrafo|\n",
    "|`<br>`        |\tInserta un salto de línea (line break) |\n",
    "|`<!--...-->`\t |  Define un comentario|\n",
    "    \n",
    "Para saber más sobre HTML podés consultar [acá](https://www.w3schools.com/TAGS/ref_byfunc.asp) la lista de etiquetas de este lenguaje.\n",
    "\n",
    "###### ¿Cómo consigo el código HTML?\n",
    "\n",
    "Ahora que sabemos cuál es el componente principal de los sitios webs podemos intentar programar a nuestra computadora para leer HTML y extraer información útil.\n",
    "\n",
    "Para conseguir el código de un sitio web podemos:\n",
    "- Ir a herramientas del desarrollador (`ctrl+shift+i`) en el navegador.\n",
    "- Presionar `ctrl+u` en el navegador.\n",
    "\n",
    "Para hacer lo mismo desde Python podemos usar la librería requests (vamos a verlo ahora)\n",
    "\n",
    "### Primer ejemplo: títulos de noticias\n",
    "\n",
    "#### **Método: BeautifulSoup**\n",
    "* Esta librería provee un *parser* de html, o sea un programa que analiza/entiende el código. Así, nos permite hacer consultas más sofisticadas de forma simple, por ejemplo: \"buscar todos los títulos h2 del sitio\".\n",
    "\n",
    "* Se usa para extraer los datos de archivos HTML. Crea un árbol de análisis a partir del código fuente de la página que se puede utilizar para extraer datos de forma jerárquica y más legible.\n",
    "\n",
    "Empecemos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b96d8f-f723-45c4-8e1a-353bc71b2867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install requests\n",
    "#pip install BeautifulSoup\n",
    "#pip install pandas\n",
    "# Nota: si no tienen instaladas las librarías a importar debajo, primero deben instalarlas\n",
    "# (para eso, quiten el # y activen las 3 líneas de código de arriba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14368fc6-c141-4006-be14-e7ae5bafb4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #html requestor\n",
    "from bs4 import BeautifulSoup #html parser\n",
    "import pandas as pd #dataframe manipulator\n",
    "\n",
    "url = \"https://www.lanacion.com.ar/\"\n",
    "\n",
    "r = requests.get(url) #traigo el contenido del html\n",
    "contenido = r.content\n",
    "\n",
    "soup = BeautifulSoup(contenido, \"html.parser\")\n",
    "soup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3dc0ad-8264-412f-bc62-3cd8534b4ff3",
   "metadata": {},
   "source": [
    "Seleccionando un título que aparece bajo la etiqueta o tag h2, vemos, por ejemplo:\n",
    "Nota: esto cambia según el día en que hagan el request o pedido (ya que la página de noticias se actualiza)\n",
    "\n",
    "<h2 class=\"text ln-text title --prumo --font-medium --font-m-l\"><span class=\"text ln-text lead --prumo --font-extra\">Tras la reunión.<!-- --> </span>Un gobernador reveló el piso de Ganancias y la fórmula jubilatoria que propone el Gobierno</h2>\n",
    "\n",
    "### Opción A - Usando find y find_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2ddfbb-0fb7-4a72-bc2d-b79a5d8f5a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dentro de la sopa, busco los elementos que contienen la información que necesito\n",
    "# Buscamos el elemento <h2> indicando la clase (class). Escribo class_ porque \"class\" es una palabra reservada en Python\n",
    "h2_element = soup.find('h2', class_='text ln-text title --prumo --font-medium --font-m-l')\n",
    "\n",
    "# Obtenemos el texto del elemento <h2>\n",
    "h2_text = h2_element.text.strip()  # strip() permite remover espacios sobrantes\n",
    "\n",
    "print(h2_element)\n",
    "print('\\n', h2_text)\n",
    "\n",
    "# Obtuvimos el *primer* elemento de la página con ese tag. Pero queremos hacerlo para todos los títulos...\n",
    "# El método \"find_all\" busca TODOS los elementos de la página con ese tag y devuelve una lista que los contiene\n",
    "# (en realidad devuelve un objeto de la clase \"bs4.element.ResultSet\")\n",
    "h2_elements = soup.find_all('h2')\n",
    "\n",
    "print(type(h2_elements))\n",
    "print('\\n', h2_elements)\n",
    "\n",
    "# Extraemos el texto de cada elemento <h2> e imprimimos\n",
    "for h2_element in h2_elements:\n",
    "    h2_text = h2_element.text.strip()\n",
    "    print(h2_text)\n",
    "    print(\" \")\n",
    "\n",
    "# Aclaración: el nombre del ítem por el que iteramos puede ser el que nosotros queramos, por ejemplo: i\n",
    "for i in h2_elements:\n",
    "    h2_text = i.text.strip()\n",
    "    print(h2_text)\n",
    "\n",
    "# Idealmente, tenemos que guardar estos títulos, queremos analizarlos\n",
    "titulos = [] # primero creamos una lista\n",
    "\n",
    "# Extraemos el texto de cada elemento <h2> y ahora guardamos\n",
    "for h2_element in h2_elements:\n",
    "    h2_text = h2_element.text.strip()\n",
    "    #print(h2_text)\n",
    "\n",
    "    titulos.append({\n",
    "        'titular': h2_text\n",
    "    })\n",
    "\n",
    "# Creamos un dataframe a partir de la lista de títulos\n",
    "titulos_df = pd.DataFrame(titulos)\n",
    "\n",
    "titulos_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f008783-196f-41b8-94f7-87994bed0bf6",
   "metadata": {},
   "source": [
    "### Opción B - Usando select. Defino un selector\n",
    "\n",
    "Un selector es un descriptor de un elemento de HTML. Como antes usamos la etiqueta (tag) h2 para encontrar los elementos buscados, un selector combina etiquetas, clases y ids en un solo string para hacer la búsqueda deseada.\n",
    "\n",
    "Si quieren [acá](https://www.w3schools.com/cssref/css_selectors.php) tienen un enlace para leer más sobre selectores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19115bf6-2c31-4909-b128-3936a22a6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos un selector de los objetos que englobe la información buscada\n",
    "# Vamos a tratar de ser más precisos: vimos que los títulos están bajo los tags h2, pero ahora\n",
    "# buscaremos indicar en qué objeto se encontraban estos tags h2\n",
    "# (y así indicar dónde encontrar los h2 con más precisión)\n",
    "\n",
    "# En este caso los titulares se encuentran en diferentes \"description containers\"\n",
    "desc_selector = '.description-container'; # Identifica todos los elementos HTML con la clase 'description container'.\n",
    "# Nota: Antes de la clase se pone un punto\n",
    "\n",
    "# Con el método select y el selector especificado buscamos todos los elementos deseados\n",
    "desc_elements = soup.select(desc_selector) # Nuevamente, obtenemos un \"bs4.element.ResultSet\"\n",
    "\n",
    "print(type(desc_elements)) # bs4.element.ResultSet\n",
    "print('\\n Primer elemento de los resultados:\\n', desc_elements[0])\n",
    "print('\\n', type(desc_elements[0])) # bs4.element.Tag\n",
    "\n",
    "# Una vez obtenidas ya las cajas con los titulares, buscamos los elementos dentro de cada una que contengan los títulos\n",
    "# Definimos un selector de estos elementos\n",
    "h2_selector = 'h2' # Identifica todos los elementos HTML cuyo tag sea 'h2'\n",
    "\n",
    "print(desc_elements[1])\n",
    "print('\\n', desc_elements[1].select(h2_selector))\n",
    "\n",
    "# Creamos una lista vacía para guardar los títulos\n",
    "titulos_sel = []\n",
    "\n",
    "# Realizamos un loop por todas las descripciones encontradas. Y por cada una, ejecutamos una consulta local\n",
    "# a cada elemento, buscando entonces el título de cada noticia con el selector ya definido\n",
    "for desc in desc_elements :\n",
    "    # Nos quedamos con el 1er resultado, ya que asumimos que nuestro selector es suficientemente preciso\n",
    "    # como para encontrar solo resultados válidos.\n",
    "    h2_element = desc.select(h2_selector)[0]\n",
    "    titular = h2_element.get_text() # Obtenemos el texto\n",
    "\n",
    "    titulos_sel.append({\n",
    "        'titular': titular\n",
    "    })\n",
    "\n",
    "# Creamos un dataframe con los títulos a partir de la lista\n",
    "titulos_sel_df = pd.DataFrame(titulos_sel)\n",
    "\n",
    "titulos_sel_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2cd2ef-a319-4a6c-9299-c82a3d27b540",
   "metadata": {},
   "source": [
    "### Análisis de sentimiento de los títulos de noticias\n",
    "\n",
    "Más información sobre sentiment analysis, acá: https://www.datacamp.com/tutorial/text-analytics-beginners-nltk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec18b3b-fbab-4aa5-91cd-1dcd7ccf1a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si aún no instalaron estas librerías, activar estas líneas de código -quitar #- para instalarlas\n",
    "#pip install string\n",
    "#pip install pandas\n",
    "#pip install nltk\n",
    "#pip install stop_words\n",
    "#pip install spacy\n",
    "#!python -m spacy download es_core_news_sm\n",
    "#pip uninstall vaderSentiment\n",
    "#pip install vader-multi\n",
    "\n",
    "# Importamos los paquetes a utilizar\n",
    "import string\n",
    "import pandas as pd\n",
    "import nltk # para procesamiento del lenguaje natural\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy # para preprocesamiento en español\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# ntlk requiere descargar algunos datos adicionales\n",
    "#nltk.download('all')\n",
    "\n",
    "# Para trabajar en inglés usar:\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2ee743-61bc-429c-b90b-7177558a477f",
   "metadata": {},
   "source": [
    "Vamos a limpiar los títulos\n",
    "Como parte del preprocesamiento de la información tenemos los siguientes pasos:\n",
    "1. Tokenization: Implica dividir el texto en palabras (o tokens)\n",
    "2. Eliminar stop words: quitar palabras comunes e irrelevantes que no tienen mucho \"sentimiento\". Esto permite mejorar la precisión del análisis de sentimiento\n",
    "3. Lemmatization: reducir las palabras a sus raíces (por ejemplo, eliminando sufijos, pasar de \"leyendo\" a \"leer\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e8aa7c-8731-4379-b229-cc56a1eb075f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos la lista de signos de puntuación\n",
    "print(string.punctuation)\n",
    "# Como estamos trabajando en español, es conveniente agregar algunos símbolos más a los signos de puntuación\n",
    "string.punctuation = string.punctuation + '¿¡“”'\n",
    "print(string.punctuation)\n",
    "\n",
    "# Cargar palabras vacías en español\n",
    "stop_words = get_stop_words('spanish')\n",
    "print(stop_words)\n",
    "\n",
    "def limpiar_titulos_nltk(titulo):\n",
    "    '''\n",
    "    Esta función limpia el texto del título.\n",
    "    Convierte texto en tokens, elimina stop words, y transforma palabras en su forma raíz\n",
    "    para dejar en el texto solo las palabras con mayor contenido.\n",
    "    Input:\n",
    "        título (str): Texto del título original\n",
    "    Output:\n",
    "        título (str): Texto del título limpio\n",
    "    '''\n",
    "\n",
    "    # 1. Separar los títulos en tokens (obtenemos una lista con palabras)\n",
    "    word_tokens = word_tokenize(titulo.lower())\n",
    "\n",
    "    # 2. Eliminar palabras vacías (stop words) de los títulos\n",
    "    # Loop por las condiciones\n",
    "    filtered_tokens = []\n",
    "    for w in word_tokens:\n",
    "        # Verificamos tokens contra stop words y puntuación\n",
    "        if w not in stop_words and w not in string.punctuation:\n",
    "            filtered_tokens.append(w)\n",
    "\n",
    "    # 3. Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    lemmatized_tokens = []\n",
    "    for w in filtered_tokens:\n",
    "        lemmatizer.lemmatize(w)\n",
    "        lemmatized_tokens.append(w)\n",
    "\n",
    "    # Volvemos a armar la oración (concatenamos las palabras separándolas con un espacio)\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Cargar el modelo para el español y las stop words según spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "stopwords_spacy = spacy.lang.es.stop_words.STOP_WORDS\n",
    "\n",
    "def limpiar_titulos_spacy(titulo):\n",
    "    '''\n",
    "    Esta función limpia el texto del título (usando funcionalidades de la librería spacy).\n",
    "    Convierte texto en tokens, elimina stop words, y transforma palabras en su forma raíz\n",
    "    para dejar en el texto solo las palabras con mayor contenido.\n",
    "    Input:\n",
    "        título (str): Texto del título original\n",
    "    Output:\n",
    "        título (str): Texto del título limpio\n",
    "    '''\n",
    "\n",
    "    # Procesar el texto con spaCy\n",
    "    doc = nlp(titulo.lower())\n",
    "    #print(doc)\n",
    "\n",
    "    filtered_tokens = []\n",
    "    lemmas = []\n",
    "\n",
    "    # Pasar a tokens y eliminar puntación y stopwords\n",
    "    for w in doc:\n",
    "        if w.text not in stopwords_spacy and not w.is_punct:\n",
    "            filtered_tokens.append(w.text)\n",
    "    filtered_doc = ' '.join(filtered_tokens)\n",
    "    #print(filtered_doc)\n",
    "\n",
    "    # Obtener las formas lematizadas de las palabras\n",
    "    doc2 = nlp(filtered_doc)\n",
    "    for w_f in doc2:\n",
    "        lemmas.append(w_f.lemma_)\n",
    "\n",
    "    # Volvemos a armar la oración (concatenamos las palabras separándolas con un espacio)\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "#Este es un título sucio:\n",
    "titulos_sel[0]\n",
    "\n",
    "#Este es un título limpio:\n",
    "limpiar_titulos_nltk(titulos_sel[0]['titular'])\n",
    "\n",
    "#Este es un título limpio:\n",
    "limpiar_titulos_spacy(titulos_sel[0]['titular'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7b33fb-6ba1-4cfe-9d7e-143e6fe35aa1",
   "metadata": {},
   "source": [
    "#### Ahora vamos a usar sentiment analysis para ver qué tan positivos son los títulos\n",
    "\n",
    "Vamos a usar la bilioteca NLTK (Natural Language Toolkit) para clasificar los títulos en positivos o negativos. NLTK es una biblioteca de Python muy utilizada en procesamiento de lenguaje natural (NLP). VADER (Valence Aware Dictionary and Sentiment Reasoner) es un módulo específico de NLTK que se utiliza para el análisis de sentimientos.\n",
    "\n",
    "VADER es una herramienta especialmente diseñada para el análisis de sentimientos en textos. A diferencia de algunos enfoques más generales que utilizan modelos de aprendizaje automático, VADER se basa en un conjunto de reglas y un diccionario que asigna puntuaciones de polaridad a palabras y expresiones. Además, tiene en cuenta factores como las mayúsculas, los signos de puntuación y los emoticonos para evaluar la intensidad del sentimiento.\n",
    "\n",
    "Las puntuaciones de VADER incluyen la polaridad (positiva, negativa o neutra) y una medida de la intensidad del sentimiento. Es especialmente útil para textos informales o con lenguaje coloquial, como se encuentra comúnmente en redes sociales.\n",
    "\n",
    "Vamos a probar dos módulos con VADER:\n",
    "1. con VADER \"original\". Ver: https://www.nltk.org/_modules/nltk/sentiment/vader.html\n",
    "2. con VADER \"multi\". Ver: https://github.com/brunneis/vader-multi\n",
    "En realidad, ambos buscan hacer los mismo pero los modelos de fondo están entrenados de formas distintas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682eaae6-4daf-4a97-92d1-aa3b635b31b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Inicializar el analizador de sentimientos VADER\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Primero veamos un ejemplo\n",
    "texto_ej_pos = \"Me encanta este curso, es genial\"\n",
    "texto_ej_neg = \"Odio este curso, es terrible\"\n",
    "texto_ej_neu = \"Me da igual el curso\"\n",
    "\n",
    "print(texto_ej_pos, sia.polarity_scores(texto_ej_pos))\n",
    "print(texto_ej_neg, sia.polarity_scores(texto_ej_neg))\n",
    "print(texto_ej_neu, sia.polarity_scores(texto_ej_neu))\n",
    "# Si la variable compound es positiva, el texto es positivo; si es negativa, el texto es negativo\n",
    "# Y si se encuentra en el rango del 0 es un mensaje neutro\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as SentimentIntensityAnalyzerMulti\n",
    "\n",
    "# Inicializar el analizador de sentimientos VADER\n",
    "sia2 = SentimentIntensityAnalyzerMulti()\n",
    "\n",
    "# Primero veamos un ejemplo\n",
    "texto_ej_pos = \"Me encanta este curso, es genial\"\n",
    "texto_ej_neg = \"Odio este curso, es terrible\"\n",
    "texto_ej_neu = \"Me da igual el curso\"\n",
    "\n",
    "print(texto_ej_pos, sia2.polarity_scores(texto_ej_pos))\n",
    "print(texto_ej_neg, sia2.polarity_scores(texto_ej_neg))\n",
    "print(texto_ej_neu, sia2.polarity_scores(texto_ej_neu))\n",
    "# Si la variable compound es positiva, el texto es positivo; si es negativa, el texto es negativo\n",
    "# Y si se encuentra en el rango del 0 es un mensaje neutro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f61847-3cf0-4006-ac84-18581571c03c",
   "metadata": {},
   "source": [
    "Ahora crearemos funciones que, además de dar un valor, clasifiquen en positivo, negativo o neutro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec81b1d-295b-495b-8ab6-c49e1959e7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analizar_sentiment(text):\n",
    "    # Obtener la polaridad del sentimiento\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    sentiment_score = sia.polarity_scores(text)\n",
    "\n",
    "    # Clasificar el sentimiento como positivo, negativo o neutro\n",
    "    compound_score = sentiment_score['compound']\n",
    "    if compound_score >= 0.05:\n",
    "        sentiment = \"Positivo\"\n",
    "    elif compound_score <= -0.05:\n",
    "        sentiment = \"Negativo\"\n",
    "    else:\n",
    "        sentiment = \"Neutro\"\n",
    "\n",
    "    return compound_score, sentiment\n",
    "\n",
    "def analizar_sentiment2(text):\n",
    "    # Obtener la polaridad del sentimiento\n",
    "    sia2 = SentimentIntensityAnalyzerMulti()\n",
    "    sentiment_score = sia2.polarity_scores(text)\n",
    "\n",
    "    # Clasificar el sentimiento como positivo, negativo o neutro\n",
    "    compound_score = sentiment_score['compound']\n",
    "    if compound_score >= 0.05:\n",
    "        sentiment = \"Positivo\"\n",
    "    elif compound_score <= -0.05:\n",
    "        sentiment = \"Negativo\"\n",
    "    else:\n",
    "        sentiment = \"Neutro\"\n",
    "\n",
    "    return compound_score, sentiment\n",
    "\n",
    "# Ejemplos de uso\n",
    "compound_score_pos, sentiment_pos = analizar_sentiment(texto_ej_pos)\n",
    "print(f\"Texto: {texto_ej_pos}\")\n",
    "print(f\"Puntuación de sentimiento compuesta: {compound_score_pos}\")\n",
    "print(f\"Sentimiento: {sentiment_pos}\")\n",
    "\n",
    "compound_score_neg, sentiment_neg = analizar_sentiment(texto_ej_neg)\n",
    "print(f\"\\nTexto: {texto_ej_neg}\")\n",
    "print(f\"Puntuación de sentimiento compuesta: {compound_score_neg}\")\n",
    "print(f\"Sentimiento: {sentiment_neg}\")\n",
    "\n",
    "# Ejemplos de uso\n",
    "compound_score_pos, sentiment_pos = analizar_sentiment2(texto_ej_pos)\n",
    "print(f\"Texto: {texto_ej_pos}\")\n",
    "print(f\"Puntuación de sentimiento compuesta: {compound_score_pos}\")\n",
    "print(f\"Sentimiento: {sentiment_pos}\")\n",
    "\n",
    "compound_score_neg, sentiment_neg = analizar_sentiment2(texto_ej_neg)\n",
    "print(f\"\\nTexto: {texto_ej_neg}\")\n",
    "print(f\"Puntuación de sentimiento compuesta: {compound_score_neg}\")\n",
    "print(f\"Sentimiento: {sentiment_neg}\")\n",
    "\n",
    "# Ahora un ejemplo con un título limpio\n",
    "print(titulos_sel[0]['titular'],\n",
    "      \"\\n\",\n",
    "      analizar_sentiment(limpiar_titulos_spacy(titulos_sel[0]['titular'])))\n",
    "\n",
    "# Aplicamos la función para limpiar títulos para tener un columna con títulos limpios\n",
    "titulos_sel_df['titular_limpio'] = titulos_sel_df['titular'].apply(limpiar_titulos_nltk)\n",
    "# Vemos el sentiment\n",
    "titulos_sel_df['sentiment'] = titulos_sel_df['titular_limpio'].apply(analizar_sentiment)\n",
    "titulos_sel_df['sentiment2'] = titulos_sel_df['titular_limpio'].apply(analizar_sentiment2)\n",
    "\n",
    "titulos_sel_df\n",
    "\n",
    "titulos_sel_df['sentiment'].value_counts()\n",
    "\n",
    "titulos_sel_df['sentiment2'].value_counts()\n",
    "\n",
    "titulos_sel_df.to_excel('titulos.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1735f9b-c001-4e97-bf26-8af73de86bc9",
   "metadata": {},
   "source": [
    "#### Un ejemplo en inglés usando TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fcff15-1646-4a11-aa76-baf0c61ee9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Ejemplo de texto en inglés\n",
    "texto_ej_1 = \"I love learning about Big data\"\n",
    "texto_ej_2 = \"I hate learning about Big data\"\n",
    "\n",
    "# Crear un objeto TextBlob con el texto\n",
    "blob1 = TextBlob(texto_ej_1)\n",
    "\n",
    "# Obtener la polaridad del sentimiento (-1 a 1)\n",
    "polarity1 = blob1.sentiment.polarity\n",
    "\n",
    "# Clasificar el sentimiento como positivo, negativo o neutro\n",
    "def clasif_polarity(polarity):\n",
    "    if polarity > 0:\n",
    "        sentiment = \"Positivo\"\n",
    "    elif polarity < 0:\n",
    "        sentiment = \"Negativo\"\n",
    "    else:\n",
    "        sentiment = \"Neutro\"\n",
    "    return sentiment\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(f\"Texto: {texto_ej_1}\")\n",
    "print(f\"Polaridad del sentimiento: {polarity1}\")\n",
    "print(f\"Sentimiento: {clasif_polarity(polarity1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1ffee9-fe04-4d29-95aa-97ca59a3e15c",
   "metadata": {},
   "source": [
    "### Otro Ejemplo de Web Scraping: Tabla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c572adac-b835-490e-96cb-a005153cab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests #html requestor\n",
    "from bs4 import BeautifulSoup #html parser\n",
    "import pandas as pd #dataframe manipulator\n",
    "\n",
    "url = 'https://datatables.net/examples/basic_init/zero_configuration.html'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635fee54-6793-401f-855b-ac685b33b78d",
   "metadata": {},
   "source": [
    "Solicitamos el html del url indicado. El código de respuesta 200 significa que la respuesta del sitio fue exitosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a388fe6a-8e68-42ed-87d1-c488cb7e4bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9151e64e-692e-45d5-a78d-3ccac14bcec7",
   "metadata": {},
   "source": [
    "Dividimos el texto con `BeautifulSoup`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872dd4ee-00ba-4378-b379-4216bdbdc8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fc2377-8d21-4538-9ca1-01f9a39fc9e2",
   "metadata": {},
   "source": [
    "Se puede observar que la informacion que queremos extraer esta entre las etiquetas llamadas `tr`. Por lo tanto queremos encontrar todas las etiquetas `tr`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbfef88-0728-4cf8-8017-341e4e040607",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = soup.find_all('tr')\n",
    "info\n",
    "\n",
    "print(type(info))\n",
    "print(type(info[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fcc5df-d940-4b78-842d-abf20ba94d63",
   "metadata": {},
   "source": [
    "Podemos ver que todas las observaciones, excepto la primera y la última, contienen la información que necesitamos. También observamos que el nombre, el puesto, el cargo, la edad, la fecha de inicio y el salario siempre tienen el mismo orden. Podemos hacer uso de estos patrones para extraer la información en un marco de datos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c9afdf-0040-4260-9d91-4247016b359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['name', 'position', 'office', 'age', 'start date', 'salary'])\n",
    "\n",
    "print(info[1].find_all('td'))\n",
    "\n",
    "for i, item in enumerate(info): #enumerate da la posición\n",
    "    if i != 0 and i != len(info)-1:\n",
    "        datos_de_fila = item.find_all('td')\n",
    "        fila = []\n",
    "        for dato in datos_de_fila:\n",
    "            fila.append(dato.text)\n",
    "        print('\\nFila:', i)\n",
    "        print(fila)\n",
    "        df.loc[i-1] = fila # mencionamos el nombre/etiqueta de las filas que queremos seleccionar\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ceb595-aad1-4031-a882-ad954c98f862",
   "metadata": {},
   "source": [
    "Por último exportamos la información a un archivo `csv`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401fc8e3-c048-4223-9651-89f75947d6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('final_output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
